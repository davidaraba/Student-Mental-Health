{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6c45",
   "metadata": {},
   "source": [
    "# Predicting Student Depression: A Big Data Analytics Approach with Apache Spark\n",
    "\n",
    "- **Author:** David Araba\n",
    "- **Student ID:** 48093143\n",
    "- **Course:** INFS3208 - Cloud Computing\n",
    "- **Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255577",
   "metadata": {},
   "source": [
    "## 1. Introduction & Project Goals\n",
    "\n",
    "### 1.1. Objective\n",
    "The primary objective of this project is to develop and evaluate a suite of machine learning models using Apache Spark to predict the likelihood of depression among students. By leveraging a comprehensive dataset that includes demographic, academic, and lifestyle factors, I aim to identify key indicators associated with mental health challenges in an academic environment.\n",
    "\n",
    "### 1.2. Significance\n",
    "Student mental health is a growing concern globally. The pressures of academic life, combined with financial and social stressors, can significantly impact a student's well-being and academic performance. This project is important because it seeks to create a data-driven framework that could potentially identify at-risk students, enabling educational institutions to offer timely and targeted support. By using scalable cloud computing technologies, we can build a foundation for a system capable of handling large-scale, real-world student data, moving from reactive to proactive mental wellness strategies. [cite_start]This aligns with the need for modern solutions that traditional computing can struggle to scale effectively[cite: 1].\n",
    "\n",
    "### 1.3. Technical Stack\n",
    "This project will be implemented using the following technologies:\n",
    "* **Language:** Python 3.x\n",
    "* **Core Engine:** Apache Spark (via PySpark)\n",
    "* **Libraries:**\n",
    "    * **Spark MLlib:** For building scalable machine learning pipelines.\n",
    "    * **Pandas:** For initial data handling and manipulation.\n",
    "    * **Matplotlib & Seaborn:** For data visualisation and result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbdb",
   "metadata": {},
   "source": [
    "## 2. Project Architecture & Workflow\n",
    "\n",
    "\n",
    "### 2.1. Workflow Description\n",
    "This project follows a standard big data analytics workflow, as depicted in the diagram below. The process begins with the ingestion of four separate but related data files into the Spark environment. These datasets are then joined and pre-processed to create a unified, analysis-ready master dataset. Subsequently, this dataset is used to train and evaluate four distinct machine learning functionalities as required by the project specification: classification, regression, clustering, and association rule mining. The final insights and model performance metrics are then visualised to provide clear, interpretable results.\n",
    "\n",
    "### 2.2. Architecture Diagram\n",
    "This diagram illustrates the project's workflow from data source to final analysis. It explicitly shows the use of multiple data sources and the application of various Spark MLlib functionalities, fulfilling the key project requirements.\n",
    "\n",
    "\n",
    "### 2.3. Architecture Explanation\n",
    "The above workflow diagram demonstrates a comprehensive big data analytics pipeline designed for scalable student mental health analysis. The process begins with four distinct data sources being ingested simultaneously into the Spark environment, leveraging Spark's distributed processing capabilities to handle large-scale datasets efficiently. The unified master dataset is then processed through feature engineering pipelines before being split for model training and evaluation. The four ML functionalities operate in parallel, each addressing different aspects of student mental health prediction and analysis. This architecture showcases the power of cloud computing technologies in handling complex, multi-dimensional data analysis tasks that would be challenging with traditional single-machine approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27c77a",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "This section prepares the notebook's environment. The first part imports all necessary libraries for the project, including `pyspark` for distributed data processing, `pyspark.ml` for machine learning, and `matplotlib` for visualisation. The second part initialises the `SparkSession`, which is the essential entry point to all of Spark's functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460345b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/17 20:00:47 WARN Utils: Your hostname, Davids-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.55 instead (on interface en0)\n",
      "25/10/17 20:00:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/17 20:00:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully. Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Project Initialisation & Overview\n",
    "# 3. Environment Setup\n",
    "\n",
    "# 3.1. Import All Necessary Libraries\n",
    "# ---\n",
    "\n",
    "# Spark libraries for session management, data manipulation, and ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, avg, mean, array\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "\n",
    "# Standard Python libraries for data handling and plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# 3.2. Initialise Spark Session\n",
    "# ---\n",
    "# Create a SparkSession, which is the entry point to any Spark functionality.\n",
    "# - appName: Sets a name for the application, which will appear in the Spark UI.\n",
    "# - getOrCreate(): Gets an existing SparkSession or, if there is none, creates a new one.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentMentalHealthPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created successfully. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8994e8",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section focuses on the initial steps of our data science workflow as outlined in the project structure. We will load our datasets, merge them into a unified DataFrame, conduct an initial inspection to understand its structure and quality, and then perform exploratory visualizations to uncover key patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224c780",
   "metadata": {},
   "source": [
    "### 1.1 Ingest and Merge Datasets\n",
    "\n",
    "First, we load the four separate CSV files (`student_info.csv`, `academic_data.csv`, `lifestyle_data.csv`, and `mental_health.csv`) into individual Spark DataFrames. We then perform a series of inner joins on the `id` column to create a single, unified master dataset for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e72fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged all data sources. Displaying a sample:\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "| id|Gender| Age|         City|Profession| Degree|CGPA|Academic Pressure|Work Pressure|Study Satisfaction|Job Satisfaction|Work/Study Hours|     Sleep Duration|Dietary Habits|Financial Stress|Family History of Mental Illness|Have you ever had suicidal thoughts ?|Depression|\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "|  2|  Male|33.0|Visakhapatnam|   Student|B.Pharm|8.97|              5.0|          0.0|               2.0|             0.0|             3.0|        '5-6 hours'|       Healthy|             1.0|                              No|                                  Yes|         1|\n",
      "|  8|Female|24.0|    Bangalore|   Student|    BSc| 5.9|              2.0|          0.0|               5.0|             0.0|             3.0|        '5-6 hours'|      Moderate|             2.0|                             Yes|                                   No|         0|\n",
      "| 26|  Male|31.0|     Srinagar|   Student|     BA|7.03|              3.0|          0.0|               5.0|             0.0|             9.0|'Less than 5 hours'|       Healthy|             1.0|                             Yes|                                   No|         0|\n",
      "| 30|Female|28.0|     Varanasi|   Student|    BCA|5.59|              3.0|          0.0|               2.0|             0.0|             4.0|        '7-8 hours'|      Moderate|             5.0|                             Yes|                                  Yes|         1|\n",
      "| 32|Female|25.0|       Jaipur|   Student| M.Tech|8.13|              4.0|          0.0|               3.0|             0.0|             1.0|        '5-6 hours'|      Moderate|             1.0|                              No|                                  Yes|         0|\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "The master DataFrame contains 27901 rows.\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths relative to the 'notebooks' directory\n",
    "path_info = '../data/student_info.csv'\n",
    "path_academic = '../data/academic_data.csv'\n",
    "path_lifestyle = '../data/lifestyle_data.csv'\n",
    "path_mental = '../data/mental_health.csv'\n",
    "\n",
    "# Load each CSV file into a separate Spark DataFrame\n",
    "sdf_info = spark.read.csv(path_info, header=True, inferSchema=True)\n",
    "sdf_academic = spark.read.csv(path_academic, header=True, inferSchema=True)\n",
    "sdf_lifestyle = spark.read.csv(path_lifestyle, header=True, inferSchema=True)\n",
    "sdf_mental = spark.read.csv(path_mental, header=True, inferSchema=True)\n",
    "\n",
    "# Perform a series of inner joins on the 'id' column\n",
    "master_df = sdf_info.join(sdf_academic, \"id\", \"inner\") \\\n",
    "                    .join(sdf_lifestyle, \"id\", \"inner\") \\\n",
    "                    .join(sdf_mental, \"id\", \"inner\")\n",
    "\n",
    "# Show the first 5 rows and verify the total count to confirm the merge\n",
    "print(\"Successfully merged all data sources. Displaying a sample:\")\n",
    "master_df.show(5)\n",
    "print(f\"The master DataFrame contains {master_df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e443371",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Inspection\n",
    "\n",
    "Now that the data is merged, we perform an initial inspection to understand its structure and identify potential data quality issues. We use `.printSchema()` to review column names and data types and `.describe().show()` to get a statistical summary of the numerical columns. This step is crucial for spotting inconsistencies, such as numerical data being incorrectly read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a276978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the master DataFrame:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Degree: string (nullable = true)\n",
      " |-- CGPA: double (nullable = true)\n",
      " |-- Academic Pressure: double (nullable = true)\n",
      " |-- Work Pressure: double (nullable = true)\n",
      " |-- Study Satisfaction: double (nullable = true)\n",
      " |-- Job Satisfaction: double (nullable = true)\n",
      " |-- Work/Study Hours: double (nullable = true)\n",
      " |-- Sleep Duration: string (nullable = true)\n",
      " |-- Dietary Habits: string (nullable = true)\n",
      " |-- Financial Stress: string (nullable = true)\n",
      " |-- Family History of Mental Illness: string (nullable = true)\n",
      " |-- Have you ever had suicidal thoughts ?: string (nullable = true)\n",
      " |-- Depression: integer (nullable = true)\n",
      "\n",
      "\n",
      "Statistical summary of numerical features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 20:00:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "|summary|               id|Gender|              Age|         City|      Profession|    Degree|              CGPA| Academic Pressure|       Work Pressure|Study Satisfaction|    Job Satisfaction|  Work/Study Hours|Sleep Duration|Dietary Habits|  Financial Stress|Family History of Mental Illness|Have you ever had suicidal thoughts ?|        Depression|\n",
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "|  count|            27901| 27901|            27901|        27901|           27901|     27901|             27901|             27901|               27901|             27901|               27901|             27901|         27901|         27901|             27901|                           27901|                                27901|             27901|\n",
      "|   mean| 70442.1494211677|  NULL|25.82230027597577|          3.0|            NULL|      NULL|7.6561041718936975|3.1412135765743163|4.300921113938568...| 2.943837138453819|6.809791763736067E-4| 7.156983620658758|          NULL|          NULL|3.1398666571080365|                            NULL|                                 NULL|0.5854987276441704|\n",
      "| stddev|40641.17521639801|  NULL|4.905687448924319|         NULL|            NULL|      NULL| 1.470707346207614|1.3814648413275878| 0.04399203206392464| 1.361147955266989|0.044394396218606996|3.7076420727787696|          NULL|          NULL|1.4373466968345752|                            NULL|                                 NULL|0.4926445636931096|\n",
      "|    min|                2|Female|             18.0| 'Less Delhi'|'Civil Engineer'|'Class 12'|               0.0|               0.0|                 0.0|               0.0|                 0.0|               0.0|   '5-6 hours'|       Healthy|               1.0|                              No|                                   No|                 0|\n",
      "|    max|           140699|  Male|             59.0|Visakhapatnam|         Teacher|       PhD|              10.0|               5.0|                 5.0|               5.0|                 4.0|              12.0|        Others|     Unhealthy|                 ?|                             Yes|                                  Yes|                 1|\n",
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the schema to check column names and data types\n",
    "print(\"Schema of the master DataFrame:\")\n",
    "master_df.printSchema()\n",
    "\n",
    "# Get a statistical summary of the numerical features\n",
    "print(\"\\nStatistical summary of numerical features:\")\n",
    "master_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab8091",
   "metadata": {},
   "source": [
    "The schema and summary statistics immediately highlight a data quality issue. While most columns are correctly typed, the `.describe()` output shows a `'?'` value in the `max` row for `Financial Stress`, confirming it was incorrectly read as a `string`. This prevents proper statistical analysis and must be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc9c87",
   "metadata": {},
   "source": [
    "### 1.3 Data Type Correction and Quality Check\n",
    "\n",
    "The initial inspection confirmed that columns intended to be numerical, such as `Financial Stress`, were incorrectly inferred as a `string` type due to `'?'` placeholder values. To rectify this, we will explicitly cast all numerical columns to a `double` type. Using `try_cast` is essential as it will gracefully convert these placeholders into `NULL` values.\n",
    "\n",
    "After this correction, we'll perform a systematic check for null values to see the full extent of the missing data that needs to be handled in the pre-processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba112c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data types corrected successfully.\n",
      "\n",
      "Verifying the new schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Degree: string (nullable = true)\n",
      " |-- CGPA: double (nullable = true)\n",
      " |-- Academic Pressure: double (nullable = true)\n",
      " |-- Work Pressure: double (nullable = true)\n",
      " |-- Study Satisfaction: double (nullable = true)\n",
      " |-- Job Satisfaction: double (nullable = true)\n",
      " |-- Work/Study Hours: double (nullable = true)\n",
      " |-- Sleep Duration: string (nullable = true)\n",
      " |-- Dietary Habits: string (nullable = true)\n",
      " |-- Financial Stress: double (nullable = true)\n",
      " |-- Family History of Mental Illness: string (nullable = true)\n",
      " |-- Have you ever had suicidal thoughts ?: string (nullable = true)\n",
      " |-- Depression: integer (nullable = true)\n",
      "\n",
      "\n",
      "Checking for missing values after type correction:\n",
      "              Column  Null_Count\n",
      "14  Financial Stress           3\n",
      "\n",
      "⚠️ Missing values were found. These will be handled in the pre-processing step.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Type Correction and Column Definitions ---\n",
    "numerical_cols = [\n",
    "    'Age', 'CGPA', 'Academic Pressure', 'Work Pressure',\n",
    "    'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', 'Financial Stress'\n",
    "]\n",
    "\n",
    "# Create a copy of the master DataFrame to work with\n",
    "df_corrected = master_df\n",
    "\n",
    "# Explicitly cast numerical columns to a Double type, converting non-numeric values to NULL\n",
    "for column in numerical_cols:\n",
    "    df_corrected = df_corrected.withColumn(column, \n",
    "        expr(f\"try_cast(`{column}` as double)\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Data types corrected successfully.\")\n",
    "print(\"\\nVerifying the new schema:\")\n",
    "df_corrected.printSchema()\n",
    "\n",
    "# --- Data Quality Check ---\n",
    "# Now, check for nulls on the CORRECTED DataFrame\n",
    "null_counts = [(c, df_corrected.where(col(c).isNull()).count()) for c in df_corrected.columns]\n",
    "\n",
    "# Filter and display only columns that have at least one null value\n",
    "null_df = pd.DataFrame(null_counts, columns=['Column', 'Null_Count'])\n",
    "print(\"\\nChecking for missing values after type correction:\")\n",
    "print(null_df[null_df['Null_Count'] > 0])\n",
    "\n",
    "if null_df['Null_Count'].sum() == 0:\n",
    "    print(\"\\n✅ Great! No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Missing values were found. These will be handled in the pre-processing step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01c501",
   "metadata": {},
   "source": [
    "The output confirms the data type correction was successful, with the schema now showing `Financial Stress` as a `double`. The subsequent null check demonstrates the effect of `try_cast`: the 3 non-numeric placeholder values have been correctly identified and converted into `NULL`. This provides a precise count of the missing data that must be addressed in the pre-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a06e6",
   "metadata": {},
   "source": [
    "### 1.4 Exploratory Visualisation\n",
    "\n",
    "With our data loaded and inspected, we now create visualizations to better understand the distributions of key features. This includes analyzing the balance of our target variable ('Depression') and exploring the distributions of important demographic and academic features. For ease of plotting, we convert the cleansed Spark DataFrame (`df_corrected`) to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target Variable Distribution ---\n",
    "\n",
    "# Aggregate the data in Spark to count occurrences of each class in the 'Depression' column\n",
    "depression_counts = df_corrected.groupBy('Depression').count().toPandas()\n",
    "\n",
    "# Map the numerical labels to meaningful names for the plot\n",
    "depression_counts['Depression'] = depression_counts['Depression'].map({0: 'Not Depressed', 1: 'Depressed'})\n",
    "\n",
    "# Create the plot using Seaborn with specified colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Define a custom color palette\n",
    "custom_palette = {'Depressed': 'red', 'Not Depressed': 'blue'}\n",
    "sns.barplot(x='Depression', y='count', data=depression_counts, palette=custom_palette, order=['Depressed', 'Not Depressed'])\n",
    "plt.title('Distribution of Student Depression Status', fontsize=16)\n",
    "plt.xlabel('Status', fontsize=12)\n",
    "plt.ylabel('Number of Students', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4e8bc",
   "metadata": {},
   "source": [
    "The bar chart of our target variable reveals a moderate class imbalance. There are more students classified as 'Depressed' than 'Not Depressed' in this dataset. This is a critical observation that will influence how we evaluate our classification models later, as simple accuracy could be a misleading metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corrected Spark DataFrame to a Pandas DataFrame for easier plotting\n",
    "pandas_df = df_corrected.toPandas()\n",
    "\n",
    "# --- Create a figure with multiple subplots for feature distributions ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Distribution of Key Demographic and Academic Features', fontsize=18)\n",
    "\n",
    "# 1. Gender Distribution (Code updated to resolve FutureWarning)\n",
    "sns.countplot(ax=axes[0], x='Gender', data=pandas_df, hue='Gender', palette='viridis', legend=False)\n",
    "axes[0].set_title('Gender Distribution')\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# 2. Age Distribution\n",
    "sns.histplot(ax=axes[1], x='Age', data=pandas_df, kde=True, color='skyblue', bins=30)\n",
    "axes[1].set_title('Age Distribution of Students')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. CGPA Distribution\n",
    "sns.histplot(ax=axes[2], x='CGPA', data=pandas_df, kde=True, color='salmon')\n",
    "axes[2].set_title('Distribution of CGPA')\n",
    "axes[2].set_xlabel('CGPA')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c2a11",
   "metadata": {},
   "source": [
    "These visualizations provide further insights into the dataset's composition:\n",
    "* **Gender:** The dataset contains a higher number of male participants than female participants.\n",
    "* **Age:** The age distribution is concentrated between 18 and 30 years, as expected for a student population, with several distinct peaks.\n",
    "* **CGPA:** The CGPA distribution is highly unusual. It is left-skewed with sharp, distinct peaks at integer and half-integer values (e.g., 7.0, 7.5, 8.0). This strongly suggests that CGPA was either self-reported with rounding or originates from a discrete grading system, rather than being a continuous measure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
