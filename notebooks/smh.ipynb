{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6c45",
   "metadata": {},
   "source": [
    "# Predicting Student Depression: A Big Data Analytics Approach with Apache Spark\n",
    "\n",
    "- **Author:** David Araba\n",
    "- **Student ID:** 48093143\n",
    "- **Course:** INFS3208 - Cloud Computing\n",
    "- **Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255577",
   "metadata": {},
   "source": [
    "## 1. Introduction & Project Goals\n",
    "\n",
    "### 1.1. Objective\n",
    "The primary objective of this project is to develop and evaluate a suite of machine learning models using Apache Spark to predict the likelihood of depression among students. By leveraging a comprehensive dataset that includes demographic, academic, and lifestyle factors, I aim to identify key indicators associated with mental health challenges in an academic environment.\n",
    "\n",
    "### 1.2. Significance\n",
    "Student mental health is a growing concern globally. The pressures of academic life, combined with financial and social stressors, can significantly impact a student's well-being and academic performance. This project is important because it seeks to create a data-driven framework that could potentially identify at-risk students, enabling educational institutions to offer timely and targeted support. By using scalable cloud computing technologies, we can build a foundation for a system capable of handling large-scale, real-world student data, moving from reactive to proactive mental wellness strategies. [cite_start]This aligns with the need for modern solutions that traditional computing can struggle to scale effectively[cite: 1].\n",
    "\n",
    "### 1.3. Technical Stack\n",
    "This project will be implemented using the following technologies:\n",
    "* **Language:** Python 3.x\n",
    "* **Core Engine:** Apache Spark (via PySpark)\n",
    "* **Libraries:**\n",
    "    * **Spark MLlib:** For building scalable machine learning pipelines.\n",
    "    * **Pandas:** For initial data handling and manipulation.\n",
    "    * **Matplotlib & Seaborn:** For data visualisation and result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbdb",
   "metadata": {},
   "source": [
    "## 2. Project Architecture & Workflow\n",
    "\n",
    "\n",
    "### 2.1. Workflow Description\n",
    "This project follows a standard big data analytics workflow, as depicted in the diagram below. The process begins with the ingestion of four separate but related data files into the Spark environment. These datasets are then joined and pre-processed to create a unified, analysis-ready master dataset. Subsequently, this dataset is used to train and evaluate four distinct machine learning functionalities as required by the project specification: classification, regression, clustering, and association rule mining. The final insights and model performance metrics are then visualised to provide clear, interpretable results.\n",
    "\n",
    "### 2.2. Architecture Diagram\n",
    "This diagram illustrates the project's workflow from data source to final analysis. It explicitly shows the use of multiple data sources and the application of various Spark MLlib functionalities, fulfilling the key project requirements.\n",
    "\n",
    "\n",
    "### 2.3. Architecture Explanation\n",
    "The above workflow diagram demonstrates a comprehensive big data analytics pipeline designed for scalable student mental health analysis. The process begins with four distinct data sources being ingested simultaneously into the Spark environment, leveraging Spark's distributed processing capabilities to handle large-scale datasets efficiently. The unified master dataset is then processed through feature engineering pipelines before being split for model training and evaluation. The four ML functionalities operate in parallel, each addressing different aspects of student mental health prediction and analysis. This architecture showcases the power of cloud computing technologies in handling complex, multi-dimensional data analysis tasks that would be challenging with traditional single-machine approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27c77a",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "This section prepares the notebook's environment. The first part imports all necessary libraries for the project, including `pyspark` for distributed data processing, `pyspark.ml` for machine learning, and `matplotlib` for visualisation. The second part initialises the `SparkSession`, which is the essential entry point to all of Spark's functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460345b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/17 19:18:17 WARN Utils: Your hostname, Davids-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.55 instead (on interface en0)\n",
      "25/10/17 19:18:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/17 19:18:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully. Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Project Initialisation & Overview\n",
    "# 3. Environment Setup\n",
    "\n",
    "# 3.1. Import All Necessary Libraries\n",
    "# ---\n",
    "\n",
    "# Spark libraries for session management, data manipulation, and ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, avg, mean, array\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "\n",
    "# Standard Python libraries for data handling and plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# 3.2. Initialise Spark Session\n",
    "# ---\n",
    "# Create a SparkSession, which is the entry point to any Spark functionality.\n",
    "# - appName: Sets a name for the application, which will appear in the Spark UI.\n",
    "# - getOrCreate(): Gets an existing SparkSession or, if there is none, creates a new one.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentMentalHealthPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created successfully. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8994e8",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section focuses on the initial steps of our data science workflow as outlined in the project structure. We will load our datasets, merge them into a unified DataFrame, conduct an initial inspection to understand its structure and quality, and then perform exploratory visualizations to uncover key patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224c780",
   "metadata": {},
   "source": [
    "### 1.1 Ingest and Merge Datasets\n",
    "\n",
    "First, we load the four separate CSV files (`student_info.csv`, `academic_data.csv`, `lifestyle_data.csv`, and `mental_health.csv`) into individual Spark DataFrames. We then perform a series of inner joins on the `id` column to create a single, unified master dataset for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e72fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged all data sources. Displaying a sample:\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "| id|Gender| Age|         City|Profession| Degree|CGPA|Academic Pressure|Work Pressure|Study Satisfaction|Job Satisfaction|Work/Study Hours|     Sleep Duration|Dietary Habits|Financial Stress|Family History of Mental Illness|Have you ever had suicidal thoughts ?|Depression|\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "|  2|  Male|33.0|Visakhapatnam|   Student|B.Pharm|8.97|              5.0|          0.0|               2.0|             0.0|             3.0|        '5-6 hours'|       Healthy|             1.0|                              No|                                  Yes|         1|\n",
      "|  8|Female|24.0|    Bangalore|   Student|    BSc| 5.9|              2.0|          0.0|               5.0|             0.0|             3.0|        '5-6 hours'|      Moderate|             2.0|                             Yes|                                   No|         0|\n",
      "| 26|  Male|31.0|     Srinagar|   Student|     BA|7.03|              3.0|          0.0|               5.0|             0.0|             9.0|'Less than 5 hours'|       Healthy|             1.0|                             Yes|                                   No|         0|\n",
      "| 30|Female|28.0|     Varanasi|   Student|    BCA|5.59|              3.0|          0.0|               2.0|             0.0|             4.0|        '7-8 hours'|      Moderate|             5.0|                             Yes|                                  Yes|         1|\n",
      "| 32|Female|25.0|       Jaipur|   Student| M.Tech|8.13|              4.0|          0.0|               3.0|             0.0|             1.0|        '5-6 hours'|      Moderate|             1.0|                              No|                                  Yes|         0|\n",
      "+---+------+----+-------------+----------+-------+----+-----------------+-------------+------------------+----------------+----------------+-------------------+--------------+----------------+--------------------------------+-------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "The master DataFrame contains 27901 rows.\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths relative to the 'notebooks' directory\n",
    "path_info = '../data/student_info.csv'\n",
    "path_academic = '../data/academic_data.csv'\n",
    "path_lifestyle = '../data/lifestyle_data.csv'\n",
    "path_mental = '../data/mental_health.csv'\n",
    "\n",
    "# Load each CSV file into a separate Spark DataFrame\n",
    "sdf_info = spark.read.csv(path_info, header=True, inferSchema=True)\n",
    "sdf_academic = spark.read.csv(path_academic, header=True, inferSchema=True)\n",
    "sdf_lifestyle = spark.read.csv(path_lifestyle, header=True, inferSchema=True)\n",
    "sdf_mental = spark.read.csv(path_mental, header=True, inferSchema=True)\n",
    "\n",
    "# Perform a series of inner joins on the 'id' column\n",
    "master_df = sdf_info.join(sdf_academic, \"id\", \"inner\") \\\n",
    "                    .join(sdf_lifestyle, \"id\", \"inner\") \\\n",
    "                    .join(sdf_mental, \"id\", \"inner\")\n",
    "\n",
    "# Show the first 5 rows and verify the total count to confirm the merge\n",
    "print(\"Successfully merged all data sources. Displaying a sample:\")\n",
    "master_df.show(5)\n",
    "print(f\"The master DataFrame contains {master_df.count()} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e443371",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Inspection\n",
    "\n",
    "Now that the data is merged, we perform an initial inspection to understand its structure and identify potential data quality issues. We use `.printSchema()` to review column names and data types and `.describe().show()` to get a statistical summary of the numerical columns. This step is crucial for spotting inconsistencies, such as numerical data being incorrectly read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a276978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the master DataFrame:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- Degree: string (nullable = true)\n",
      " |-- CGPA: double (nullable = true)\n",
      " |-- Academic Pressure: double (nullable = true)\n",
      " |-- Work Pressure: double (nullable = true)\n",
      " |-- Study Satisfaction: double (nullable = true)\n",
      " |-- Job Satisfaction: double (nullable = true)\n",
      " |-- Work/Study Hours: double (nullable = true)\n",
      " |-- Sleep Duration: string (nullable = true)\n",
      " |-- Dietary Habits: string (nullable = true)\n",
      " |-- Financial Stress: string (nullable = true)\n",
      " |-- Family History of Mental Illness: string (nullable = true)\n",
      " |-- Have you ever had suicidal thoughts ?: string (nullable = true)\n",
      " |-- Depression: integer (nullable = true)\n",
      "\n",
      "\n",
      "Statistical summary of numerical features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 19:20:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "|summary|               id|Gender|              Age|         City|      Profession|    Degree|              CGPA| Academic Pressure|       Work Pressure|Study Satisfaction|    Job Satisfaction|  Work/Study Hours|Sleep Duration|Dietary Habits|  Financial Stress|Family History of Mental Illness|Have you ever had suicidal thoughts ?|        Depression|\n",
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "|  count|            27901| 27901|            27901|        27901|           27901|     27901|             27901|             27901|               27901|             27901|               27901|             27901|         27901|         27901|             27901|                           27901|                                27901|             27901|\n",
      "|   mean| 70442.1494211677|  NULL|25.82230027597577|          3.0|            NULL|      NULL|7.6561041718936975|3.1412135765743163|4.300921113938568...| 2.943837138453819|6.809791763736067E-4| 7.156983620658758|          NULL|          NULL|3.1398666571080365|                            NULL|                                 NULL|0.5854987276441704|\n",
      "| stddev|40641.17521639801|  NULL|4.905687448924319|         NULL|            NULL|      NULL| 1.470707346207614|1.3814648413275878| 0.04399203206392464| 1.361147955266989|0.044394396218606996|3.7076420727787696|          NULL|          NULL|1.4373466968345752|                            NULL|                                 NULL|0.4926445636931096|\n",
      "|    min|                2|Female|             18.0| 'Less Delhi'|'Civil Engineer'|'Class 12'|               0.0|               0.0|                 0.0|               0.0|                 0.0|               0.0|   '5-6 hours'|       Healthy|               1.0|                              No|                                   No|                 0|\n",
      "|    max|           140699|  Male|             59.0|Visakhapatnam|         Teacher|       PhD|              10.0|               5.0|                 5.0|               5.0|                 4.0|              12.0|        Others|     Unhealthy|                 ?|                             Yes|                                  Yes|                 1|\n",
      "+-------+-----------------+------+-----------------+-------------+----------------+----------+------------------+------------------+--------------------+------------------+--------------------+------------------+--------------+--------------+------------------+--------------------------------+-------------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the schema to check column names and data types\n",
    "print(\"Schema of the master DataFrame:\")\n",
    "master_df.printSchema()\n",
    "\n",
    "# Get a statistical summary of the numerical features\n",
    "print(\"\\nStatistical summary of numerical features:\")\n",
    "master_df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
