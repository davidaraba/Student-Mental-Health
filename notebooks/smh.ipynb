{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6c45",
   "metadata": {},
   "source": [
    "# Predicting Student Depression: A Big Data Analytics Approach with Apache Spark\n",
    "\n",
    "- **Author:** David Araba\n",
    "- **Student ID:** 48093143\n",
    "- **Course:** INFS3208 - Cloud Computing\n",
    "- **Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255577",
   "metadata": {},
   "source": [
    "## 1. Introduction & Project Goals\n",
    "\n",
    "### 1.1. Objective\n",
    "The primary objective of this project is to develop and evaluate a suite of machine learning models using Apache Spark to predict the likelihood of depression among students. By leveraging a comprehensive dataset that includes demographic, academic, and lifestyle factors, I aim to identify key indicators associated with mental health challenges in an academic environment.\n",
    "\n",
    "### 1.2. Significance\n",
    "Student mental health is a growing concern globally. The pressures of academic life, combined with financial and social stressors, can significantly impact a student's well-being and academic performance. This project is important because it seeks to create a data-driven framework that could potentially identify at-risk students, enabling educational institutions to offer timely and targeted support. By using scalable cloud computing technologies, we can build a foundation for a system capable of handling large-scale, real-world student data, moving from reactive to proactive mental wellness strategies. This aligns with the need for modern solutions that traditional computing can struggle to scale effectively.\n",
    "\n",
    "### 1.3. Technical Stack\n",
    "This project will be implemented using the following technologies:\n",
    "* **Language:** Python 3.x\n",
    "* **Core Engine:** Apache Spark (via PySpark)\n",
    "* **Libraries:**\n",
    "    * **Spark MLlib:** For building scalable machine learning pipelines.\n",
    "    * **Pandas:** For initial data handling and manipulation.\n",
    "    * **Matplotlib & Seaborn:** For data visualisation and result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbdb",
   "metadata": {},
   "source": [
    "## 2. Project Architecture & Workflow\n",
    "\n",
    "\n",
    "### 2.1. Workflow Description\n",
    "This project follows a standard big data analytics workflow, as depicted in the diagram below. The process begins with the ingestion of four separate but related data files into the Spark environment. These datasets are then joined and pre-processed to create a unified, analysis-ready master dataset. Subsequently, this dataset is used to train and evaluate four distinct machine learning functionalities as required by the project specification: classification, regression, clustering, and association rule mining. The final insights and model performance metrics are then visualised to provide clear, interpretable results.\n",
    "\n",
    "### 2.2. Architecture Diagram\n",
    "This diagram illustrates the project's workflow from data source to final analysis. It explicitly shows the use of multiple data sources and the application of various Spark MLlib functionalities, fulfilling the key project requirements.\n",
    "\n",
    "\n",
    "### 2.3. Architecture Explanation\n",
    "The above workflow diagram demonstrates a comprehensive big data analytics pipeline designed for scalable student mental health analysis. The process begins with four distinct data sources being ingested simultaneously into the Spark environment, leveraging Spark's distributed processing capabilities to handle large-scale datasets efficiently. The unified master dataset is then processed through feature engineering pipelines before being split for model training and evaluation. The four ML functionalities operate in parallel, each addressing different aspects of student mental health prediction and analysis. This architecture showcases the power of cloud computing technologies in handling complex, multi-dimensional data analysis tasks that would be challenging with traditional single-machine approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27c77a",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "This section prepares the notebook's environment. The first part imports all necessary libraries for the project, including `pyspark` for distributed data processing, `pyspark.ml` for machine learning, and `matplotlib` for visualisation. The second part initialises the `SparkSession`, which is the essential entry point to all of Spark's functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460345b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully and matplotlib is set to inline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/19 16:15:11 WARN Utils: Your hostname, Davids-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.55 instead (on interface en0)\n",
      "25/10/19 16:15:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/david/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/david/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/david/.ivy2.5.2/jars\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19b1a578-0836-41d0-8f18-cf6c0110f0fc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.20 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;2.0.1 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.15 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.guava#guava;32.1.2-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.33.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.61.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound io.grpc#grpc-api;1.61.0 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;2.2.20 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.4 in central\n",
      "\tfound com.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20240105-2.0.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.22.0 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.22.0 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.3 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;2.8 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.3 in central\n",
      "\tfound com.google.flogger#google-extensions;0.7.1 in central\n",
      "\tfound com.google.flogger#flogger;0.7.1 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.3 in central\n",
      "\tfound com.google.flogger#flogger-system-backend;0.7.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.20 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;2.2.20 in central\n",
      "\tfound io.grpc#grpc-alts;1.61.0 in central\n",
      "\tfound io.grpc#grpc-auth;1.61.0 in central\n",
      "\tfound io.grpc#grpc-core;1.61.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.61.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.61.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.25.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.29.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.61.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.61.0 in central\n",
      "\tfound io.grpc#grpc-census;1.61.0 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.32.1 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.24.1 in central\n",
      "\tfound commons-codec#commons-codec;1.16.0 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.3 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.5.4 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.25.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.1.7 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.31.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.3 in central\n",
      "\tfound com.google.api#gax-httpjson;2.41.0 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.31.0 in central\n",
      "\tfound com.google.api#gax;2.41.0 in central\n",
      "\tfound com.google.api#gax-grpc;2.42.0 in central\n",
      "\tfound io.grpc#grpc-inprocess;1.61.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.8 in central\n",
      "\tfound com.google.api#api-common;2.24.0 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.32.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.32.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.32.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.32.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.16.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      "\tfound com.google.cloud#google-cloud-storage-control;2.32.1-alpha in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-control-v2;2.32.1-alpha in central\n",
      "\tfound io.grpc#grpc-xds;1.61.0 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-util;1.61.0 in central\n",
      "\tfound io.grpc#grpc-services;1.61.0 in central\n",
      "\tfound com.google.re2j#re2j;1.7 in central\n",
      "\tfound io.opencensus#opencensus-impl;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-impl-core;0.31.0 in central\n",
      "\tfound com.lmax#disruptor;3.4.2 in central\n",
      "\tfound io.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-exemplar-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-resource-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-exporter-metrics-util;0.31.0 in central\n",
      "\tfound com.google.cloud#google-cloud-monitoring;1.82.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-grpc-metrics;0.31.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.23 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.61.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.61.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.27.0 in central\n",
      "\tfound io.grpc#grpc-rls;1.61.0 in central\n",
      ":: resolution report :: resolve 1416ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.16.1 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.24.0 from central in [default]\n",
      "\tcom.google.api#gax;2.41.0 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.42.0 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;2.41.0 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.32.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.32.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-control-v2;2.32.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.32.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.32.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.1.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20240105-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.22.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.22.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.5.4 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.31.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.31.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-monitoring;1.82.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.32.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage-control;2.32.1-alpha from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.20 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;2.2.20 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;2.2.20 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.20 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.24.1 from central in [default]\n",
      "\tcom.google.flogger#flogger;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#flogger-system-backend;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#google-extensions;0.7.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;32.1.2-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;2.8 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.25.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.25.2 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.7 from central in [default]\n",
      "\tcom.lmax#disruptor;3.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-census;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-inprocess;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-rls;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-util;1.61.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.61.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-exemplar-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-grpc-metrics;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-resource-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-metrics-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl-core;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.27.0 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.23 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.8 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.api-client#google-api-client;2.0.1 by [com.google.api-client#google-api-client;2.2.0] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 by [com.google.http-client#google-http-client;1.43.3] in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.15 by [org.apache.httpcomponents#httpcore;4.4.16] in [default]\n",
      "\tcommons-codec#commons-codec;1.15 by [commons-codec#commons-codec;1.16.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 by [com.google.errorprone#error_prone_annotations;2.18.0] in [default]\n",
      "\torg.checkerframework#checker-qual;3.33.0 by [org.checkerframework#checker-qual;3.42.0] in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 by [com.google.j2objc#j2objc-annotations;2.8] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 by [com.google.http-client#google-http-client-gson;1.43.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 by [com.google.http-client#google-http-client-apache-v2;1.43.3] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.4 by [com.fasterxml.jackson.core#jackson-core;2.16.1] in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.0 by [com.google.api-client#google-api-client;2.2.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 by [com.google.errorprone#error_prone_annotations;2.24.1] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.29.0 by [com.google.api.grpc#proto-google-common-protos;2.32.0] in [default]\n",
      "\tcom.google.api#gax;2.12.2 by [com.google.api#gax;2.41.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.11.0 by [com.google.errorprone#error_prone_annotations;2.18.0] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.7.3 by [com.google.api.grpc#proto-google-common-protos;2.32.0] in [default]\n",
      "\torg.threeten#threetenbp;1.5.2 by [org.threeten#threetenbp;1.6.8] in [default]\n",
      "\tcom.google.api#api-common;2.1.4 by [com.google.api#api-common;2.24.0] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.41.3 by [com.google.http-client#google-http-client;1.43.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.41.3 by [com.google.http-client#google-http-client-gson;1.43.3] in [default]\n",
      "\tcom.google.api#gax;2.42.0 by [com.google.api#gax;2.41.0] in [default]\n",
      "\tcom.google.api#api-common;2.25.0 by [com.google.api#api-common;2.24.0] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.33.0 by [com.google.api.grpc#proto-google-common-protos;2.32.0] in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.0 by [io.opencensus#opencensus-api;0.31.1] in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;1.82.0 by [com.google.cloud#google-cloud-core-grpc;2.31.0] in [default]\n",
      "\tcom.google.api#api-common;1.8.1 by [com.google.api#api-common;2.24.0] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;1.16.0 by [com.google.api.grpc#proto-google-common-protos;2.32.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.23.0 by [com.google.errorprone#error_prone_annotations;2.24.1] in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 by [io.perfmark#perfmark-api;0.27.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  117  |   0   |   0   |   29  ||   88  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tcircular dependency found: io.grpc#grpc-core;1.61.0->io.grpc#grpc-util;1.61.0->...\n",
      "\n",
      "\tcircular dependency found: io.grpc#grpc-util;1.61.0->io.grpc#grpc-core;1.61.0->...\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19b1a578-0836-41d0-8f18-cf6c0110f0fc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 88 already retrieved (0kB/14ms)\n",
      "25/10/19 16:15:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully. Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1. Import All Necessary Libraries ---\n",
    "\n",
    "# Standard Python libraries for data handling and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# IMPORTANT: Add this magic command to render plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, avg, mean, array\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "print(\"Libraries imported successfully and matplotlib is set to inline.\")\n",
    "\n",
    "# --- 3.2. Initialise Spark Session with Full GCS Configuration ---\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentMentalHealthPrediction\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.20\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.type\", \"APPLICATION_DEFAULT_CREDENTIALS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created successfully. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8994e8",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section focuses on the initial steps of our data science workflow as outlined in the project structure. We will load our datasets, merge them into a unified DataFrame, conduct an initial inspection to understand its structure and quality, and then perform exploratory visualizations to uncover key patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224c780",
   "metadata": {},
   "source": [
    "### 1.1 Ingest and Merge Datasets\n",
    "\n",
    "First, we load the four separate CSV files (`student_info.csv`, `academic_data.csv`, `lifestyle_data.csv`, and `mental_health.csv`) into individual Spark DataFrames. We then perform a series of inner joins on the `id` column to create a single, unified master dataset for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e72fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reading data from Google Cloud Storage bucket: david-araba-infs3208-data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/david/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/david/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/david/miniconda3/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyboardInterrupt\n",
      "25/10/19 16:18:26 WARN FileSystem: Failed to initialize filesystem gs://david-araba-infs3208-data/student_info.csv: java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
      "25/10/19 16:18:26 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://david-araba-infs3208-data/student_info.csv.\n",
      "java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n",
      "\tat com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254)\n",
      "\tat com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Connect timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:546)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:592)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:751)\n",
      "\tat java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:531)\n",
      "\tat java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:636)\n",
      "\tat java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:280)\n",
      "\tat java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)\n",
      "\tat java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1304)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1237)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1123)\n",
      "\tat java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1052)\n",
      "\tat com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:151)\n",
      "\tat com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84)\n",
      "\tat com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012)\n",
      "\tat com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:196)\n",
      "\tat com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470)\n",
      "\tat com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251)\n",
      "\t... 87 more\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# --- Load Data from the Selected Source ---\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     sdf_info = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     sdf_academic = spark.read.csv(path_academic, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     27\u001b[39m     sdf_lifestyle = spark.read.csv(path_lifestyle, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/pyspark/sql/readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/uni/infs3208/SMH/smh/lib/python3.13/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/19 16:18:46 WARN FileSystem: Failed to initialize filesystem gs://david-araba-infs3208-data/student_info.csv: java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1.1 Ingest and Merge Datasets ---\n",
    "\n",
    "# --- Configuration Switch ---\n",
    "# Set this to True to read from Google Cloud Storage\n",
    "# Set this to False to read from the local '../data/' directory\n",
    "USE_GCS = True\n",
    "\n",
    "# --- Define File Paths based on the Switch ---\n",
    "if USE_GCS:\n",
    "    bucket_name = \"david-araba-infs3208-data\"\n",
    "    base_path = f'gs://{bucket_name}/'\n",
    "    print(f\"âœ… Reading data from Google Cloud Storage bucket: {bucket_name}\")\n",
    "else:\n",
    "    base_path = \"../data\"\n",
    "    print(\"âœ… Reading data from local '../data/' directory.\")\n",
    "\n",
    "# Define the full paths for each file\n",
    "path_info = base_path + 'student_info.csv'\n",
    "path_academic = base_path + 'academic_data.csv'\n",
    "path_lifestyle = base_path + 'lifestyle_data.csv'\n",
    "path_mental = base_path + 'mental_health.csv'\n",
    "\n",
    "# --- Load Data from the Selected Source ---\n",
    "try:\n",
    "    sdf_info = spark.read.csv(path_info, header=True, inferSchema=True)\n",
    "    sdf_academic = spark.read.csv(path_academic, header=True, inferSchema=True)\n",
    "    sdf_lifestyle = spark.read.csv(path_lifestyle, header=True, inferSchema=True)\n",
    "    sdf_mental = spark.read.csv(path_mental, header=True, inferSchema=True)\n",
    "\n",
    "    # Perform a series of inner joins on the 'id' column\n",
    "    master_df = sdf_info.join(sdf_academic, \"id\", \"inner\") \\\n",
    "                        .join(sdf_lifestyle, \"id\", \"inner\") \\\n",
    "                        .join(sdf_mental, \"id\", \"inner\")\n",
    "\n",
    "    # Show the first 5 rows and verify the total count to confirm the merge\n",
    "    print(\"\\nSuccessfully merged all data sources. Displaying a sample:\")\n",
    "    master_df.show(5)\n",
    "    print(f\"The master DataFrame contains {master_df.count()} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nâš ï¸ An error occurred while loading the data.\")\n",
    "    if USE_GCS:\n",
    "        print(\"Please check the following:\")\n",
    "        print(\"1. Your GCS bucket name is correct.\")\n",
    "        print(\"2. You are authenticated with Google Cloud (e.g., via 'gcloud auth application-default login').\")\n",
    "        print(\"3. The CSV files exist in the bucket.\")\n",
    "    else:\n",
    "        print(\"Please check that the '../data/' directory and all CSV files exist.\")\n",
    "    print(f\"\\nError details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e443371",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Inspection\n",
    "\n",
    "Now that the data is merged, we perform an initial inspection to understand its structure and identify potential data quality issues. We use `.printSchema()` to review column names and data types and `.describe().show()` to get a statistical summary of the numerical columns. This step is crucial for spotting inconsistencies, such as numerical data being incorrectly read as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a276978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the master DataFrame:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'master_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Print the schema to check column names and data types\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSchema of the master DataFrame:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmaster_df\u001b[49m.printSchema()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Get a statistical summary of the numerical features\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStatistical summary of numerical features:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'master_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the schema to check column names and data types\n",
    "print(\"Schema of the master DataFrame:\")\n",
    "master_df.printSchema()\n",
    "\n",
    "# Get a statistical summary of the numerical features\n",
    "print(\"\\nStatistical summary of numerical features:\")\n",
    "master_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab8091",
   "metadata": {},
   "source": [
    "The schema and summary statistics immediately highlight a data quality issue. While most columns are correctly typed, the `.describe()` output shows a `'?'` value in the `max` row for `Financial Stress`, confirming it was incorrectly read as a `string`. This prevents proper statistical analysis and must be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc9c87",
   "metadata": {},
   "source": [
    "### 1.3 Data Type Correction and Quality Check\n",
    "\n",
    "The initial inspection confirmed that columns intended to be numerical, such as `Financial Stress`, were incorrectly inferred as a `string` type due to `'?'` placeholder values. To rectify this, we will explicitly cast all numerical columns to a `double` type. Using `try_cast` is essential as it will gracefully convert these placeholders into `NULL` values.\n",
    "\n",
    "After this correction, we'll perform a systematic check for null values to see the full extent of the missing data that needs to be handled in the pre-processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba112c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Type Correction and Column Definitions ---\n",
    "categorical_cols = [\n",
    "    'Gender', 'City', 'Profession', 'Degree', 'Sleep Duration', \n",
    "    'Dietary Habits', 'Have you ever had suicidal thoughts ?', \n",
    "    'Family History of Mental Illness'\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    'Age', 'CGPA', 'Academic Pressure', 'Work Pressure',\n",
    "    'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', 'Financial Stress'\n",
    "]\n",
    "\n",
    "# Create a copy of the master DataFrame to work with\n",
    "df_corrected = master_df\n",
    "\n",
    "# Explicitly cast numerical columns to a Double type, converting non-numeric values to NULL\n",
    "for column in numerical_cols:\n",
    "    df_corrected = df_corrected.withColumn(column, \n",
    "        expr(f\"try_cast(`{column}` as double)\")\n",
    "    )\n",
    "\n",
    "print(\"âœ… Data types corrected successfully.\")\n",
    "print(\"\\nVerifying the new schema:\")\n",
    "df_corrected.printSchema()\n",
    "\n",
    "# --- Data Quality Check ---\n",
    "# Now, check for nulls on the CORRECTED DataFrame\n",
    "null_counts = [(c, df_corrected.where(col(c).isNull()).count()) for c in df_corrected.columns]\n",
    "\n",
    "# Filter and display only columns that have at least one null value\n",
    "null_df = pd.DataFrame(null_counts, columns=['Column', 'Null_Count'])\n",
    "print(\"\\nChecking for missing values after type correction:\")\n",
    "print(null_df[null_df['Null_Count'] > 0])\n",
    "\n",
    "if null_df['Null_Count'].sum() == 0:\n",
    "    print(\"\\nâœ… Great! No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Missing values were found. These will be handled in the pre-processing step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01c501",
   "metadata": {},
   "source": [
    "The output confirms the data type correction was successful, with the schema now showing `Financial Stress` as a `double`. The subsequent null check demonstrates the effect of `try_cast`: the 3 non-numeric placeholder values have been correctly identified and converted into `NULL`. This provides a precise count of the missing data that must be addressed in the pre-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a06e6",
   "metadata": {},
   "source": [
    "### 1.4 Exploratory Visualisation\n",
    "\n",
    "With our data loaded and inspected, we now create visualizations to better understand the distributions of key features. This includes analyzing the balance of our target variable ('Depression') and exploring the distributions of important demographic and academic features. For ease of plotting, we convert the cleansed Spark DataFrame (`df_corrected`) to a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target Variable Distribution ---\n",
    "\n",
    "# Aggregate the data in Spark to count occurrences of each class in the 'Depression' column\n",
    "depression_counts = df_corrected.groupBy('Depression').count().toPandas()\n",
    "\n",
    "# Map the numerical labels to meaningful names for the plot\n",
    "depression_counts['Depression'] = depression_counts['Depression'].map({0: 'Not Depressed', 1: 'Depressed'})\n",
    "\n",
    "# Create the plot using Seaborn with specified colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Define a custom color palette\n",
    "custom_palette = {'Depressed': 'red', 'Not Depressed': 'blue'}\n",
    "sns.barplot(x='Depression', y='count', data=depression_counts, palette=custom_palette, order=['Depressed', 'Not Depressed'])\n",
    "plt.title('Distribution of Student Depression Status', fontsize=16)\n",
    "plt.xlabel('Status', fontsize=12)\n",
    "plt.ylabel('Number of Students', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4e8bc",
   "metadata": {},
   "source": [
    "The bar chart of our target variable reveals a moderate class imbalance. There are more students classified as 'Depressed' than 'Not Depressed' in this dataset. This is a critical observation that will influence how we evaluate our classification models later, as simple accuracy could be a misleading metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corrected Spark DataFrame to a Pandas DataFrame for easier plotting\n",
    "pandas_df = df_corrected.toPandas()\n",
    "\n",
    "# --- Create a figure with multiple subplots for feature distributions ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Distribution of Key Demographic and Academic Features', fontsize=18)\n",
    "\n",
    "# 1. Gender Distribution (Code updated to resolve FutureWarning)\n",
    "sns.countplot(ax=axes[0], x='Gender', data=pandas_df, hue='Gender', palette='viridis', legend=False)\n",
    "axes[0].set_title('Gender Distribution')\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# 2. Age Distribution\n",
    "sns.histplot(ax=axes[1], x='Age', data=pandas_df, kde=True, color='skyblue', bins=30)\n",
    "axes[1].set_title('Age Distribution of Students')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. CGPA Distribution\n",
    "sns.histplot(ax=axes[2], x='CGPA', data=pandas_df, kde=True, color='salmon')\n",
    "axes[2].set_title('Distribution of CGPA')\n",
    "axes[2].set_xlabel('CGPA')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c2a11",
   "metadata": {},
   "source": [
    "These visualizations provide further insights into the dataset's composition:\n",
    "* **Gender:** The dataset contains a higher number of male participants than female participants.\n",
    "* **Age:** The age distribution is concentrated between 18 and 30 years, as expected for a student population, with several distinct peaks.\n",
    "* **CGPA:** The CGPA distribution is highly unusual. It is left-skewed with sharp, distinct peaks at integer and half-integer values (e.g., 7.0, 7.5, 8.0). This strongly suggests that CGPA was either self-reported with rounding or originates from a discrete grading system, rather than being a continuous measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3ea28",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering & Pre-processing\n",
    "\n",
    "This section prepares our clean, explored data for the machine learning models. The primary goal is to transform the dataset into a format suitable for Spark's MLlib library. This involves three key steps:\n",
    "\n",
    "1. **Data Cleaning:** We will implement the strategy decided upon in Part 1 by handling the `NULL` values discovered in the `Financial Stress`.\n",
    "2.  **Feature Transformation:** Machine learning models require numerical inputs, so we must convert our categorical text columns (like `Gender`, `City`, etc.) into a numerical format. We will use a standard two-step process with **`StringIndexer`** and **`OneHotEncoder`**.\n",
    "3.  **Feature Assembling:** Finally, we will use **`VectorAssembler`** to combine all our individual feature columns into a single vector column. This is a mandatory step, as all Spark MLlib models expect this specific format for their input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a2d72",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning: Imputing Missing Values\n",
    "\n",
    "As identified in the EDA phase, our type correction process created three `NULL` values in the `Financial Stress` column. Leaving these as `NULL` would cause errors during model training. A common and effective strategy for handling a small number of missing numerical values is imputation. Here, we replace the `NULL` values with the statistical **mean** of the entire column, preserving the overall distribution of the data without discarding valuable rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Handle Missing Values Created from try_cast ---\n",
    "\n",
    "# Check for null values created by try_cast (from '?' values)\n",
    "null_counts = [(c, df_corrected.where(col(c).isNull()).count()) for c in numerical_cols]\n",
    "null_df = pd.DataFrame(null_counts, columns=['Column', 'Null_Count'])\n",
    "print(\"Checking for null values created from try_cast:\")\n",
    "print(null_df[null_df['Null_Count'] > 0])\n",
    "\n",
    "# Fill null values with column means\n",
    "for column in numerical_cols:\n",
    "    # Calculate mean for each column\n",
    "    mean_val = df_corrected.select(mean(col(column))).collect()[0][0]\n",
    "    if mean_val is not None:  # Only fill if mean exists\n",
    "        df_corrected = df_corrected.fillna(mean_val, subset=[column])\n",
    "        print(f\"Filled null values in {column} with mean: {mean_val:.2f}\")\n",
    "\n",
    "print(\"âœ… Data cleaning complete. All '?' values converted to column means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474e0f3",
   "metadata": {},
   "source": [
    "### 2.2 Feature Transformation: Encoding Categorical Variables\n",
    "\n",
    "Machine learning algorithms require numerical input. Therefore, we must convert our text-based categorical columns (e.g., 'Gender', 'City') into a numerical format. We use a standard two-step process for each column:\n",
    "\n",
    "1.  **`StringIndexer`**: This transformer assigns a unique numerical index to each unique category in a column (e.g., 'Male' -> 0.0, 'Female' -> 1.0).\n",
    "2.  **`OneHotEncoder`**: This transformer takes the numerical index and converts it into a sparse binary vector. This prevents the model from incorrectly assuming an ordinal relationship between categories (e.g., that 'City B' is somehow \"greater than\" 'City A').\n",
    "\n",
    "We create these two stages for every categorical column and add them to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1569a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a list of transformation stages ---\n",
    "stages = []\n",
    "\n",
    "# Iterate over each categorical column to create StringIndexer and OneHotEncoder stages\n",
    "for column in categorical_cols:\n",
    "    string_indexer = StringIndexer(inputCol=column, outputCol=column + \"_Index\")\n",
    "    one_hot_encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()], outputCols=[column + \"_Vec\"])\n",
    "    stages += [string_indexer, one_hot_encoder]\n",
    "\n",
    "print(\"StringIndexer and OneHotEncoder stages created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0ab44",
   "metadata": {},
   "source": [
    "### 2.3 Feature Assembling\n",
    "\n",
    "All Spark MLlib models expect the input features to be consolidated into a single vector column. We use `VectorAssembler` to achieve this. This final transformation stage gathers all our processed feature columnsâ€”both the original numerical columns and the new one-hot encoded vectorsâ€”and combines them into a single column named `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all feature columns (numerical + the new vector columns from OHE)\n",
    "feature_cols = numerical_cols + [c + \"_Vec\" for c in categorical_cols]\n",
    "\n",
    "# Create the VectorAssembler stage\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Add the assembler to our list of stages\n",
    "stages += [vector_assembler]\n",
    "\n",
    "print(\"VectorAssembler stage created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90999f",
   "metadata": {},
   "source": [
    "### 2.4 Executing the Pre-processing Pipeline\n",
    "\n",
    "With all our stages defined (imputation handled separately, followed by indexing, encoding, and assembling), we combine them into a single `Pipeline`. A `Pipeline` is a powerful Spark MLlib tool that chains multiple transformers and estimators together to create a unified workflow.\n",
    "\n",
    "We then `.fit()` the pipeline on our data to learn the necessary transformations (like the string-to-index mappings) and `.transform()` the data to apply them. The output below shows the final structure of our dataset, now ready for model training, with the `features` vector successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92216177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full pre-processing pipeline with all our defined stages\n",
    "preprocessing_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit the pipeline to our corrected data\n",
    "pipeline_model = preprocessing_pipeline.fit(df_corrected)\n",
    "\n",
    "# Transform the data to apply all the steps\n",
    "transformed_df = pipeline_model.transform(df_corrected)\n",
    "\n",
    "# Display the results to confirm the 'features' vector was created\n",
    "print(\"Data transformation complete. Displaying the transformed DataFrame:\")\n",
    "transformed_df.select('id', 'Depression', 'features').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b3355",
   "metadata": {},
   "source": [
    "## Part 3: Machine Learning Modelling\n",
    "\n",
    "This is the core implementation section of the project where we build, train, and evaluate our machine learning models. With our data now fully pre-processed, we will address a key assignment requirement by implementing **four distinct analytical tasks** using Spark MLlib.\n",
    "\n",
    "The goal is to analyze the student dataset from multiple perspectives to uncover different types of insights. Our four chosen functionalities, as outlined in the project proposal, are:\n",
    "\n",
    "* **1. Classification:** Predicting the likelihood of a student having depression.\n",
    "* **2. Regression:** Predicting a student's academic performance (CGPA).\n",
    "* **3. Clustering:** Identifying distinct groups or profiles of students based on their data.\n",
    "* **4. Association Rule Mining:** Discovering hidden patterns or relationships between different factors.\n",
    "\n",
    "Our first crucial step will be to split the data into training and testing sets. This ensures we can evaluate our supervised models on unseen data, providing an unbiased assessment of their predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f19eb3",
   "metadata": {},
   "source": [
    "### 3.1 Data Splitting\n",
    "\n",
    "Before we can train any supervised machine learning models, we must prepare our dataset. This involves two key actions:\n",
    "\n",
    "1.  **Renaming the Target Column:** Spark MLlib's classification and regression algorithms expect the target variable (the value we want to predict) to be in a column named `label`. We rename our `Depression` column to `label` to adhere to this convention.\n",
    "2.  **Splitting the Data:** We split our dataset into a **training set (80%)** and a **testing set (20%)**. The model will only ever learn from the training data. The testing data is held back as \"unseen\" data to provide an unbiased evaluation of how well the model generalizes to new, real-world examples. We use a `seed` to ensure this random split is the same every time we run the notebook, making our results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the target column 'Depression' to 'label'\n",
    "final_df = transformed_df.withColumnRenamed(\"Depression\", \"label\")\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Print the number of records in each set to verify the split\n",
    "print(f\"Number of training records: {train_data.count()}\")\n",
    "print(f\"Number of testing records: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc813e2",
   "metadata": {},
   "source": [
    "### 3.2 Functionality 1: Classification - Predicting Depression\n",
    "\n",
    "Our first analytical task is classification. We will train a **Logistic Regression** model to predict our binary target variable: whether a student is likely to have depression (`1`) or not (`0`). Logistic Regression is a robust and highly interpretable algorithm, making it an excellent baseline model for this kind of problem.\n",
    "\n",
    "The process involves two main steps:\n",
    "\n",
    "1.  **Training:** We initialize the `LogisticRegression` model, specifying the input `features` and `label` columns. We then train the model by calling the `.fit()` method on our `train_data`.\n",
    "2.  **Prediction:** Once the model is trained, we use the `.transform()` method on the unseen `test_data` to generate predictions.\n",
    "\n",
    "The output table shows the result. For each row in the test set, we can see the true `label`, the model's final `prediction` (0 or 1), and the `probability` vector, which indicates the model's confidence in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a49d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialize and Train the Logistic Regression Model ---\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 2. Make Predictions and Format the Output ---\n",
    "print(\"\\nMaking predictions on the test data...\")\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Create a UDF to extract the probability of the positive class (class 1)\n",
    "extract_prob = udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "# Apply the UDF to create a 'Confidence' column and round it\n",
    "predictions_formatted = predictions.withColumn(\"Confidence\", spark_round(extract_prob(\"probability\"), 4)) \\\n",
    "                                   .select(\"label\", \"prediction\", \"Confidence\")\n",
    "\n",
    "# Show the new, user-friendly table\n",
    "print(\"\\nUser-Friendly Prediction Results:\")\n",
    "predictions_formatted.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96baaa6d",
   "metadata": {},
   "source": [
    "### Understanding the Model's Predictions\n",
    "\n",
    "The table below analyzes the first 10 predictions from your test set. The key is the **Confidence** column, which represents the model's calculated probability that a student *is depressed* (the probability of `label = 1`). The model's final **Prediction** is `1.0` if this confidence is above 50% (0.5) and `0.0` if it's below.\n",
    "\n",
    "| Row | Label (True Answer) | Confidence P(Depressed) | Prediction (Model's Guess) | Result |\n",
    "| :-- | :------------------ | :---------------------- | :------------------------- | :----- |\n",
    "| 1   | 0 (Not Depressed)   | 2.77%                   | 0.0                        | âœ… Correct |\n",
    "| 2   | 0 (Not Depressed)   | 2.88%                   | 0.0                        | âœ… Correct |\n",
    "| 3   | 1 (Depressed)       | **85.31%** | 1.0                        | âœ… Correct |\n",
    "| 4   | 0 (Not Depressed)   | 23.83%                  | 0.0                        | âœ… Correct |\n",
    "| 5   | 1 (Depressed)       | 31.51%                  | 0.0                        | âŒ Incorrect |\n",
    "| 6   | 0 (Not Depressed)   | 4.72%                   | 0.0                        | âœ… Correct |\n",
    "| 7   | 1 (Depressed)       | **80.86%** | 1.0                        | âœ… Correct |\n",
    "| 8   | 0 (Not Depressed)   | 16.32%                  | 0.0                        | âœ… Correct |\n",
    "| 9   | 0 (Not Depressed)   | 9.35%                   | 0.0                        | âœ… Correct |\n",
    "| 10  | 0 (Not Depressed)   | **66.46%** | 1.0                        | âŒ Incorrect |\n",
    "\n",
    "This row-by-row analysis provides clear intuition for the model's behavior. For example, in **Row 3**, the model was **85.31%** confident the student was depressed, so it correctly predicted `1.0`, matching the true label. Conversely, in **Row 10**, the model was 66.46% confident and predicted `1.0`, but the true answer was `0`, making it confidently wrong. The next step is to aggregate these results to see the overall performance across the entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee073277",
   "metadata": {},
   "source": [
    "### 3.2.1 Evaluating the Classification Model\n",
    "\n",
    "Now that we have predictions, we need to evaluate how well our Logistic Regression model performed. We will use several standard classification metrics to get a complete picture of its strengths and weaknesses. A single metric like accuracy can be misleading, especially with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ca4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Evaluate using AUC-ROC ---\n",
    "evaluator_roc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_roc.evaluate(predictions)\n",
    "print(f\"Area Under ROC Curve (AUC): {auc:.4f}\")\n",
    "\n",
    "# --- 2. Calculate TP, TN, FP, FN from the Predictions DataFrame ---\n",
    "# These values are the building blocks for all other metrics\n",
    "tp = predictions.filter((col('label') == 1) & (col('prediction') == 1)).count()\n",
    "tn = predictions.filter((col('label') == 0) & (col('prediction') == 0)).count()\n",
    "fp = predictions.filter((col('label') == 0) & (col('prediction') == 1)).count()\n",
    "fn = predictions.filter((col('label') == 1) & (col('prediction') == 0)).count()\n",
    "\n",
    "# --- 3. Calculate and Print Key Performance Metrics ---\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n",
    "\n",
    "# --- 4. Display a Formatted Confusion Matrix ---\n",
    "# Create a more descriptive confusion matrix using Pandas for better visualization\n",
    "confusion_matrix_pd = pd.DataFrame(\n",
    "    data=[[tp, fn], [fp, tn]],\n",
    "    columns=['Predicted: Depressed', 'Predicted: Not Depressed'],\n",
    "    index=['Actual: Depressed', 'Actual: Not Depressed']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Formatted Confusion Matrix ---\")\n",
    "display(confusion_matrix_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891778d",
   "metadata": {},
   "source": [
    "### Interpreting the Model's Performance\n",
    "\n",
    "These results show our model is performing quite well as a baseline for identifying students with depression. ðŸ“ˆ\n",
    "\n",
    "#### Area Under ROC Curve (AUC): 0.9230 ðŸŽ¯\n",
    "This is an **excellent score**. The AUC measures how well the model can distinguish between a depressed and a non-depressed student. A score of 0.5 is random guessing, while 1.0 is perfect. Your score of **0.923** indicates the model has a very strong discriminative ability.\n",
    "\n",
    "---\n",
    "#### Confusion Matrix Breakdown ðŸ“Š\n",
    "The confusion matrix gives us a detailed breakdown of the model's successes and errors:\n",
    "\n",
    "* **True Positives (TP): 2,798**\n",
    "    * Students who **were depressed** and were **correctly** identified by the model. âœ…\n",
    "* **True Negatives (TN): 1,889**\n",
    "    * Students who were **not depressed** and were **correctly** identified. âœ…\n",
    "* **False Positives (FP): 486**\n",
    "    * Students who were **not depressed** but were **incorrectly** flagged as depressed (a \"false alarm\"). âŒ\n",
    "* **False Negatives (FN): 373**\n",
    "    * Students who **were depressed** but were **missed** by the model. This is the most critical error in a health context. âŒ\n",
    "\n",
    "---\n",
    "#### Key Performance Metrics\n",
    "These percentages translate the raw numbers from the matrix into performance scores:\n",
    "\n",
    "* **Accuracy: 84.51%**\n",
    "    * Overall, **84.51%** of the model's predictions were correct. It's a solid score but doesn't tell the full story.\n",
    "* **Precision: 85.20%**\n",
    "    * When the model predicted a student was depressed, it was correct **85.20%** of the time. This tells you how reliable a \"positive\" prediction is.\n",
    "* **Recall: 88.24%**\n",
    "    * Of all the students who **truly were depressed**, the model successfully identified **88.24%** of them. This is a crucial metric for this project, as a high recall means we are minimizing the number of at-risk students we miss.\n",
    "* **F1 Score: 0.8669**\n",
    "    * This is the balanced average of Precision and Recall. A score of **0.867** indicates a strong and well-balanced model, effective at handling both false positives and false negatives.\n",
    "\n",
    "The high recall is especially promising for a mental health application, though the 373 false negatives highlight an area for future improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c8b7b",
   "metadata": {},
   "source": [
    "### 3.3 Functionality 2: Regression - Predicting Academic Performance (CGPA)\n",
    "\n",
    "Our second analytical task is **regression**, where the goal is to predict a continuous numerical value instead of a categorical class. For this task, we will train a **Linear Regression** model to predict a student's **CGPA** based on all other features.\n",
    "\n",
    "This is a valuable exercise for two reasons:\n",
    "1.  **Insight Generation:** It can help us understand which lifestyle, demographic, or mental health factors are most strongly correlated with academic performance.\n",
    "2.  **Technical Requirement:** It fulfills another of the four required functionalities for this project.\n",
    "\n",
    "The process will be similar to our classification task: we'll train the model on the training data, make predictions on the test data, and then evaluate the model's performance using regression-specific metrics like Root Mean Squared Error (RMSE) and R-squared (RÂ²)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e07a54",
   "metadata": {},
   "source": [
    "### 3.3.1 Preparing Data and Training the Regression Model\n",
    "\n",
    "Before training our regression model, we must perform a crucial data preparation step to prevent **data leakage**. Our goal is to predict `CGPA`, so we must remove it from the input features. Including the target variable in the feature set would make the model's job trivial and produce misleadingly perfect results.\n",
    "\n",
    "The workflow is as follows:\n",
    "1.  **Create a New Feature Set:** We define a new list of numerical columns that excludes `CGPA`.\n",
    "2.  **Re-Assemble Features:** We use a new `VectorAssembler` to create a `regression_features` vector using this updated list.\n",
    "3.  **Prepare Final DataFrame:** We create a new DataFrame specifically for this task, selecting our new features and renaming `CGPA` to `label` for compatibility with the Spark MLlib model.\n",
    "4.  **Train and Predict:** We split this new DataFrame into training and testing sets and then fit a `LinearRegression` model to predict the `label` (CGPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare Data for Regression ---\n",
    "print(\"Preparing data for regression task...\")\n",
    "# We must exclude CGPA from our features since it's our new target variable.\n",
    "regression_numerical_cols = [\n",
    "    'Age', 'Academic Pressure', 'Work Pressure',\n",
    "    'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', 'Financial Stress'\n",
    "]\n",
    "regression_feature_cols = regression_numerical_cols + [c + \"_Vec\" for c in categorical_cols]\n",
    "regression_vector_assembler = VectorAssembler(inputCols=regression_feature_cols, outputCol=\"regression_features\")\n",
    "regression_features_df = regression_vector_assembler.transform(transformed_df)\n",
    "regression_df = regression_features_df.select(col(\"regression_features\").alias(\"features\"), col(\"CGPA\").alias(\"label\"))\n",
    "reg_train_data, reg_test_data = regression_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# --- 2. Initialize and Train the Linear Regression Model ---\n",
    "print(\"\\nTraining Linear Regression model...\")\n",
    "lr_regressor = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_regressor_model = lr_regressor.fit(reg_train_data)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 3. Make Predictions and Format the Output ---\n",
    "print(\"\\nMaking predictions on the test data...\")\n",
    "reg_predictions = lr_regressor_model.transform(reg_test_data)\n",
    "\n",
    "# Round the prediction for readability and add an 'Error' column\n",
    "reg_predictions_formatted = reg_predictions.withColumn(\"prediction_rounded\", spark_round(\"prediction\", 2))\n",
    "reg_predictions_formatted = reg_predictions_formatted.withColumn(\"Error\",\n",
    "    spark_round(spark_abs(col(\"label\") - col(\"prediction_rounded\")), 2)\n",
    ")\n",
    "\n",
    "print(\"\\n--- User-Friendly Regression Results ---\")\n",
    "reg_predictions_formatted.select(col(\"label\").alias(\"Actual_CGPA\"), col(\"prediction_rounded\").alias(\"Predicted_CGPA\"), \"Error\").show(10)\n",
    "\n",
    "# --- 4. Evaluate the Regression Model ---\n",
    "print(\"\\n--- Model Performance Metrics ---\")\n",
    "reg_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = reg_evaluator.setMetricName(\"rmse\").evaluate(reg_predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = reg_evaluator.setMetricName(\"r2\").evaluate(reg_predictions)\n",
    "print(f\"R-squared (RÂ²): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581ff8d",
   "metadata": {},
   "source": [
    "### Interpreting the Regression Results\n",
    "\n",
    "The output from our regression model provides several key insights into both the model's behavior and the nature of our data.\n",
    "\n",
    "#### Analyzing the Predictions \n",
    "The results table clearly shows the model's poor performance. Notice that the **Predicted_CGPA** values are all clustered in a very narrow range (around 7.5), regardless of the **Actual_CGPA**. For instance, it predicts a CGPA of **7.52** for a student who actually achieved a **5.58**, and a similar **7.43** for a student who achieved a **9.93**.\n",
    "\n",
    "This demonstrates that the model has simply learned to guess the average CGPA for almost everyone. It reveals a crucial finding: **the lifestyle, demographic, and mental health features in our dataset are not strong predictors of academic performance**.\n",
    "\n",
    "---\n",
    "#### Evaluating the Model with Metrics \n",
    "The formal metrics numerically confirm this observation:\n",
    "\n",
    "* **Root Mean Squared Error (RMSE): 1.4631**\n",
    "    * This means that, on average, the model's prediction for a student's CGPA is off by about **1.46 points**. On a 10-point scale, this is a very significant error, confirming the model's low accuracy.\n",
    "* **R-squared (RÂ²): 0.0168**\n",
    "    * This is the most definitive metric. An RÂ² of 0.0168 means that our model can only explain about **1.68%** of the variation in student CGPA. An RÂ² this close to zero provides strong statistical evidence that our model is performing no better than a simple baseline that just predicts the average CGPA for every student.\n",
    "\n",
    "In conclusion, while the model runs correctly, its very high RMSE and near-zero RÂ² score prove that it has almost no predictive power. This is a valuable finding in itself, demonstrating that the chosen factors are poor predictors for a student's CGPA using a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324efe47",
   "metadata": {},
   "source": [
    "### 3.4 Functionality 3: Clustering - Identifying At-Risk Student Groups \n",
    "\n",
    "Our third analytical task is **clustering**, which is a type of **unsupervised learning**. Unlike our previous models, clustering does not use a `label` column to make predictions. Instead, its goal is to discover natural groupings or \"clusters\" within the data based on the similarity of the features.\n",
    "\n",
    "For this project, we will use the **KMeans** algorithm. KMeans works by partitioning the data into a pre-defined number (`k`) of clusters, where each student belongs to the cluster with the nearest mean (a \"centroid\"). This is an excellent way to identify distinct student profiles, such as \"High-Achieving, High-Stress\" or \"Socially Engaged, Low-Pressure\" students.\n",
    "\n",
    "The primary challenge with KMeans is choosing the optimal number of clusters, `k`. To do this scientifically, we will use the **Elbow Method**. This involves running the KMeans algorithm for a range of `k` values and plotting a cost function (the Within Set Sum of Squared Errors). The \"elbow\" of the resulting curveâ€”the point where the rate of improvement slows downâ€”suggests the best value for `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500a11b",
   "metadata": {},
   "source": [
    "### 3.4.1 Finding the Optimal Number of Clusters (k) with the Elbow Method\n",
    "\n",
    "A critical parameter for the KMeans algorithm is `k`, the number of clusters. Choosing this value arbitrarily can lead to meaningless results. To make an informed decision, we will use the **Elbow Method**.\n",
    "\n",
    "This method involves running the KMeans algorithm for a range of `k` values (e.g., from 2 to 10) and calculating the **Within Set Sum of Squared Errors (WSSSE)** for each. The WSSSE measures the total distance of all points from their respective cluster centroids; a smaller WSSSE means the clusters are denser.\n",
    "\n",
    "We then plot these WSSSE values against `k`. The resulting curve typically looks like an arm. The point where the curve bendsâ€”the \"elbow\"â€”represents the point of diminishing returns, where adding more clusters no longer significantly reduces the WSSSE. This point is considered the optimal value for `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find the optimal 'k' using the Elbow Method ---\n",
    "print(\"Calculating WSSSE for a range of k values...\")\n",
    "\n",
    "# We will test k from 2 to 10\n",
    "wssse_values = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(featuresCol='features', k=k, seed=42)\n",
    "    model = kmeans.fit(final_df)\n",
    "    # The cost is the WSSSE\n",
    "    wssse = model.summary.trainingCost\n",
    "    wssse_values.append(wssse)\n",
    "    print(f\"k={k}, WSSSE={wssse:.2f}\")\n",
    "\n",
    "# --- Plot the Elbow Method Curve ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, wssse_values, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Within Set Sum of Squared Errors (WSSSE)')\n",
    "plt.title('The Elbow Method for Optimal k')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d9ac1",
   "metadata": {},
   "source": [
    "The Elbow Method plot clearly shows a distinct \"bend\" at **k=4**. After this point, the curve begins to flatten, and the reduction in WSSSE for each additional cluster becomes much less significant. This provides strong evidence that **4** is the optimal number of clusters for segmenting our student data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1db745",
   "metadata": {},
   "source": [
    "### 3.4.2 Training and Evaluating the Final KMeans Model\n",
    "\n",
    "Based on the Elbow Method analysis, we now train our final KMeans model using `k=4`. After training, we will:\n",
    "\n",
    "1.  **Assign Clusters:** Use the trained model to assign each student in our dataset to one of the four clusters.\n",
    "2.  **Evaluate Performance:** Calculate the **Silhouette Score**. This metric measures how well-separated the clusters are. A score closer to 1 is better, while a score near 0 indicates overlapping clusters.\n",
    "3.  **Analyze Cluster Sizes:** Examine the number of students assigned to each cluster to ensure the groups are of a meaningful size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c27007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialize and Train the K-Means Model ---\n",
    "# We select k=4 based on the Elbow Method plot above.\n",
    "OPTIMAL_K = 4\n",
    "print(f\"Training final K-Means model with k={OPTIMAL_K}...\")\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features\", k=OPTIMAL_K, seed=42)\n",
    "kmeans_model = kmeans.fit(final_df)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 2. Make Predictions (Assign Clusters) ---\n",
    "clustered_df = kmeans_model.transform(final_df)\n",
    "print(\"\\n--- Sample of students with their assigned cluster ---\")\n",
    "clustered_df.select(\"id\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "# --- 3. Evaluate the Clustering ---\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\")\n",
    "silhouette = evaluator.evaluate(clustered_df)\n",
    "print(f\"\\nSilhouette Score = {silhouette:.4f}\")\n",
    "\n",
    "# --- 4. Analyze Cluster Sizes ---\n",
    "print(\"\\n--- Distribution of students across clusters ---\")\n",
    "clustered_df.groupBy(\"prediction\").count().orderBy('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405cb1a",
   "metadata": {},
   "source": [
    "### Interpreting the Clustering Results\n",
    "\n",
    "Our KMeans model has successfully segmented the students into four groups, but the results suggest these groups are not perfectly distinct.\n",
    "\n",
    "* **Silhouette Score (0.3492):** This is a fair to weak score. It indicates that while the model has found some structure in the data, the four student profiles have a notable amount of overlap and are not sharply separated from one another.\n",
    "* **Cluster Distribution:** The model has created four large and relatively balanced clusters. This is a positive outcome, as it means each group is substantial enough for meaningful analysis, with no tiny, insignificant clusters.\n",
    "\n",
    "The next critical step is to perform a deeper analysis to understand the unique characteristics of each of these four clusters and define the student \"profiles\" they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849cb81",
   "metadata": {},
   "source": [
    "### 3.4.3 Analyzing Cluster Profiles\n",
    "\n",
    "The final and most important step in our clustering analysis is to interpret what each cluster represents. We do this by calculating the average value of our key features for each of the four clusters.\n",
    "\n",
    "By examining the average **Depression Rate**, **CGPA**, **Academic Pressure**, and **Financial Stress** for each group, we can build a \"profile\" or \"persona\" for the students within them. This will allow us to identify which clusters represent at-risk students and understand the factors that characterize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc553d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Calculate the Average of Key Features for Each Cluster ---\n",
    "# We group by the 'prediction' column (our cluster ID)\n",
    "# Then, we calculate the mean for our most important columns.\n",
    "cluster_profiles = clustered_df.groupBy(\"prediction\").agg(\n",
    "    avg(col(\"label\")).alias(\"Depression_Rate\"),\n",
    "    avg(col(\"CGPA\")).alias(\"Average_CGPA\"),\n",
    "    avg(col(\"Academic Pressure\")).alias(\"Avg_Academic_Pressure\"),\n",
    "    avg(col(\"Financial Stress\")).alias(\"Avg_Financial_Stress\")\n",
    ").orderBy(\"prediction\")\n",
    "\n",
    "# --- 2. Display the Results in a Clean Table ---\n",
    "print(\"--- Student Cluster Profiles ---\")\n",
    "# Convert to a Pandas DataFrame for better formatting and easier analysis\n",
    "cluster_profiles_pd = cluster_profiles.toPandas()\n",
    "display(cluster_profiles_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd152ffb",
   "metadata": {},
   "source": [
    "### Interpreting the Student Cluster Profiles \n",
    "\n",
    "This table is the key to understanding our four student clusters. By comparing the average values, we can see two distinct types of student profiles emerge: an **\"At-Risk\"** group and a **\"More Resilient\"** group.\n",
    "\n",
    "* **Clusters 1 & 3: The At-Risk Group**\n",
    "    * **High Depression Rate:** These two groups have a very high prevalence of depression, at **~69%**.\n",
    "    * **High Stressors:** This high depression rate is strongly correlated with higher average **Academic Pressure (~3.24)** and **Financial Stress (~3.28)**.\n",
    "    * **Profile:** These clusters represent students who are under significant academic and financial strain, which appears to be a major factor in their mental health.\n",
    "\n",
    "* **Clusters 0 & 2: The More Resilient Group**\n",
    "    * **Lower Depression Rate:** These groups have a significantly lower (though still notable) prevalence of depression, ranging from **42% to 49%**.\n",
    "    * **Lower Stressors:** This corresponds with lower average **Academic Pressure (~3.0)** and **Financial Stress (~2.9)**.\n",
    "    * **Profile:** These clusters represent students who experience less external pressure, which correlates with better mental health outcomes.\n",
    "\n",
    "Crucially, the **Average CGPA** is almost identical across all four clusters, suggesting that academic performance itself is not the defining characteristic of these groups. Instead, the key differentiators are the **perceived levels of stress**. This is a powerful insight, as it validates our clustering model and identifies clear, actionable factors that correlate with student depression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00702552",
   "metadata": {},
   "source": [
    "### 3.5 Functionality 4: Association Rule Mining - Discovering Hidden Patterns ðŸ”—\n",
    "\n",
    "Our fourth and final analytical task is **Association Rule Mining**, another form of unsupervised learning. The goal here is to discover hidden \"if-then\" rules or relationships within our dataset. For example, we might find a rule like `{Low Sleep, High Financial Stress} => {Depressed}`.\n",
    "\n",
    "We will use the **FPGrowth** algorithm in Spark MLlib. This algorithm requires the data to be in a specific format: each row must have a single column containing a list (or \"basket\") of categorical \"items\". To achieve this, our workflow will be:\n",
    "\n",
    "1.  **Discretize Numerical Data:** We must convert our continuous numerical features (like `CGPA` and `Academic Pressure`) into discrete categories (like 'Low_CGPA', 'High_Pressure'). We'll use the `QuantileDiscretizer` to automatically create meaningful bins (e.g., Low, Medium, High).\n",
    "2.  **Combine All Features:** We will then combine these new discretized features with our original categorical features into a single \"items\" list for each student.\n",
    "3.  **Train FPGrowth:** We'll run the FPGrowth model on this prepared data to generate the rules.\n",
    "4.  **Interpret the Rules:** We will analyze the generated rules based on three key metrics:\n",
    "    * **Antecedent (if):** The item or set of items on the left side of the rule.\n",
    "    * **Consequent (then):** The item on the right side of the rule.\n",
    "    * **Confidence:** The probability that the consequent occurs, given the antecedent. A confidence of 0.8 means the rule is correct 80% of the time.\n",
    "    * **Lift:** How much more likely the consequent is to occur when the antecedent is present. A lift greater than 1 indicates a meaningful relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare Data for FPGrowth ---\n",
    "print(\"Preparing data for FPGrowth...\")\n",
    "cols_to_discretize = ['CGPA', 'Academic Pressure', 'Financial Stress']\n",
    "discretizer_stages = []\n",
    "for col_name in cols_to_discretize:\n",
    "    discretizer = QuantileDiscretizer(numBuckets=3, inputCol=col_name, outputCol=col_name + \"_cat\", handleInvalid=\"keep\")\n",
    "    discretizer_stages.append(discretizer)\n",
    "\n",
    "discretizer_pipeline = Pipeline(stages=discretizer_stages)\n",
    "discretizer_model = discretizer_pipeline.fit(final_df)\n",
    "discretized_df = discretizer_model.transform(final_df)\n",
    "\n",
    "all_item_cols = []\n",
    "for col_name in categorical_cols:\n",
    "    expr_col = concat(lit(col_name + \"_\"), col(col_name))\n",
    "    all_item_cols.append(expr_col)\n",
    "for col_name in cols_to_discretize:\n",
    "    expr_col = concat(lit(col_name + \"_\"), col(col_name + \"_cat\"))\n",
    "    all_item_cols.append(expr_col)\n",
    "\n",
    "items_df = discretized_df.withColumn(\"items\", array(all_item_cols))\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "# --- 2. Initialize and Train the FPGrowth Model ---\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.3, minConfidence=0.6)\n",
    "print(\"\\nTraining FPGrowth model...\")\n",
    "fp_model = fpGrowth.fit(items_df)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 3. Format and Display the Association Rules ---\n",
    "print(\"\\n--- Top Association Rules (Ordered by Lift) ---\")\n",
    "association_rules = fp_model.associationRules\n",
    "rules_pd = association_rules.orderBy(col(\"lift\").desc()).limit(20).toPandas()\n",
    "\n",
    "# Define a more robust function to make the rules human-readable\n",
    "def format_rule_corrected(items):\n",
    "    level_map = {\"_0.0\": \" (Low)\", \"_1.0\": \" (Medium)\", \"_2.0\": \" (High)\"}\n",
    "    cleaned_items = []\n",
    "    for item in items:\n",
    "        # Handle specific long questions first to avoid errors\n",
    "        if \"Have you ever had suicidal thoughts ?_Yes\" in item:\n",
    "            cleaned_items.append(\"Suicidal Thoughts: Yes\")\n",
    "            continue\n",
    "        if \"Have you ever had suicidal thoughts ?_No\" in item:\n",
    "            cleaned_items.append(\"Suicidal Thoughts: No\")\n",
    "            continue\n",
    "\n",
    "        # Handle other specific prefixes\n",
    "        item = item.replace(\"Family History of Mental Illness_\", \"Family History: \")\n",
    "\n",
    "        # Replace level numbers with names for discretized columns\n",
    "        for code, name in level_map.items():\n",
    "            item = item.replace(code, name)\n",
    "\n",
    "        # General formatting\n",
    "        item = item.replace(\"_\", \" \")\n",
    "        cleaned_items.append(item)\n",
    "    return \" AND \".join(cleaned_items)\n",
    "\n",
    "# Apply the corrected formatting function\n",
    "rules_pd['antecedent'] = rules_pd['antecedent'].apply(format_rule_corrected)\n",
    "rules_pd['consequent'] = rules_pd['consequent'].apply(format_rule_corrected)\n",
    "\n",
    "# Rename columns for clarity\n",
    "rules_pd.rename(columns={'antecedent': 'IF', 'consequent': 'THEN'}, inplace=True)\n",
    "\n",
    "# Format the numeric columns\n",
    "rules_pd['confidence'] = rules_pd['confidence'].map('{:.2%}'.format)\n",
    "rules_pd['lift'] = rules_pd['lift'].map('{:.2f}'.format)\n",
    "rules_pd['support'] = rules_pd['support'].map('{:.2%}'.format)\n",
    "\n",
    "display(rules_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5f8fb",
   "metadata": {},
   "source": [
    "### Interpreting the Association Rules \n",
    "\n",
    "The FPGrowth algorithm has successfully uncovered several powerful \"if-then\" rules within our student data. The results have been formatted for clarity, with the \"if\" condition (`antecedent`) and the \"then\" outcome (`consequent`) clearly displayed.\n",
    "\n",
    "To interpret these rules, we focus on two key metrics:\n",
    "* **Confidence:** The accuracy of the rule. A confidence of 75% means that if the **IF** condition is met, the **THEN** outcome will be true 75% of the time.\n",
    "* **Lift:** The importance of the rule. A lift greater than 1 shows a real relationship. A lift of 1.19 means the **THEN** outcome is **19% more likely** to happen when the **IF** condition is true.\n",
    "\n",
    "#### Key Insights Discovered \n",
    "The most significant rules (those with the highest lift) reveal a strong link between stress and suicidal thoughts:\n",
    "\n",
    "* **Rule 1: IF a student has `High Academic Pressure`, THEN they are likely to have had `suicidal thoughts`.**\n",
    "    * **Confidence: 75.40%**. This rule is highly accurate.\n",
    "    * **Lift: 1.19**. This is a strong relationship. Students with high academic pressure are **19% more likely** than the average student to have experienced suicidal thoughts. This is a major finding.\n",
    "\n",
    "* **Rule 2: IF a student has `High Financial Stress`, THEN they are likely to have had `suicidal thoughts`.**\n",
    "    * **Confidence: 72.87%**. This rule is also very accurate.\n",
    "    * **Lift: 1.15**. Students with high financial stress are **15% more likely** to have had suicidal thoughts.\n",
    "\n",
    "#### Trivial Rules (The \"Student\" Rules)\n",
    "Many of the other rules have a lift very close to 1.0, which indicates a weak or non-existent relationship. These are considered \"trivial\" rules that confirm the structure of our data (e.g., that nearly everyone is a `Profession Student`) but do not provide new insights.\n",
    "\n",
    "**Conclusion:** The association rule mining was highly successful. It has statistically validated the powerful and actionable insight that **high academic and financial stress are strongly associated with suicidal thoughts** among students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf8e54",
   "metadata": {},
   "source": [
    "## Part 4: In-Depth Analysis & Key Findings \n",
    "\n",
    "Now that we have built and evaluated our four models, we move beyond basic implementation to extract deeper, actionable insights from our findings. This section is designed to address the \"Excellence and Innovation\" criterion by demonstrating a more profound analysis of the data.\n",
    "\n",
    "Our first step is to conduct a **Feature Importance Analysis**. The goal is to identify which factors in our dataset are the most powerful predictors of student depression. By understanding the key drivers, we can provide more targeted recommendations for potential interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af8dd1",
   "metadata": {},
   "source": [
    "### 4.1 Feature Importance Analysis\n",
    "\n",
    "To determine which features are most influential, we will extract the **coefficients** from our trained **Logistic Regression model** (`lr_model`). In a logistic regression model, the absolute magnitude of a coefficient corresponds to its importance in the prediction. A large coefficient (either positive or negative) means that a change in that feature has a strong impact on the likelihood of a student being classified as depressed.\n",
    "\n",
    "We will visualize the top 15 most important features in a bar chart to clearly see the key factors at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e337e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Extract Feature Names and Coefficients ---\n",
    "feature_attributes = transformed_df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]\n",
    "all_feature_names = [attr[\"name\"] for attr in feature_attributes[\"numeric\"]] + [attr[\"name\"] for attr in feature_attributes[\"binary\"]]\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "\n",
    "# --- 2. Create DataFrame for Analysis ---\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# --- 3. Sort and Select Top 15 Features ---\n",
    "top_15_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(15)\n",
    "\n",
    "# --- 4. Define a Function to Clean Feature Names ---\n",
    "def clean_feature_name(name):\n",
    "    # Replaces the technical prefixes with a clean, readable format\n",
    "    name = name.replace(\"City_Vec_\", \"City: \")\n",
    "    name = name.replace(\"Profession_Vec_\", \"Profession: \")\n",
    "    # Removes extra quotes that can appear\n",
    "    name = name.replace(\"'\", \"\")\n",
    "    return name\n",
    "\n",
    "# Apply the cleaning function to the 'Feature' column\n",
    "top_15_features['Feature'] = top_15_features['Feature'].apply(clean_feature_name)\n",
    "\n",
    "# --- 5. Plot the Cleaned Results ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_15_features, hue='Feature', palette='viridis', legend=False)\n",
    "plt.title('Top 15 Most Important Features for Predicting Depression', fontsize=16)\n",
    "plt.xlabel('Coefficient Magnitude (Importance)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98f73a",
   "metadata": {},
   "source": [
    "### Interpreting the Feature Importance Results ðŸŽ¯\n",
    "\n",
    "This chart reveals the most influential factors the Logistic Regression model uses to make its predictions. The results are both surprising and insightful.\n",
    "\n",
    "#### Understanding the X-Axis (Importance)\n",
    "The x-axis, **\"Coefficient Magnitude,\"** represents the \"voting power\" of each feature in the model's decision. A longer bar means the feature has a stronger influence; for example, the model pays far more attention to `City: Nalyan` (with a score over 50) than it does to `Profession: Digital Marketer` (with a score below 30).\n",
    "\n",
    "#### The Dominance of Location ðŸ—ºï¸\n",
    "The most striking finding is that **specific cities are by far the most powerful predictors of depression**. Features like `City: Nalyan` and `City: Bhavna` have the highest importance scores. This suggests that there are strong underlying geographical or socio-economic factors associated with these specific cities that are heavily correlated with student mental health in this dataset.\n",
    "\n",
    "#### The Role of Profession ðŸ§‘â€ðŸ«\n",
    "After location, a few specific professions also show some predictive power. The presence of roles like `Doctor` or `Educational Consultant` in a student dataset likely indicates that the data includes postgraduate or part-time students who are also working professionals, adding another layer of complexity to their student life.\n",
    "\n",
    "**Conclusion:** This deep-dive analysis has uncovered a critical insight that was not obvious from our initial exploration. The model has learned that **\"where you are\"** is a more significant predictor of depression than many other factors. This is a perfect example of how machine learning can identify complex patterns in data, providing a new direction for further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683f015",
   "metadata": {},
   "source": [
    "### 4.2 \"Deep Dive\" Visualisation: Stress vs. Depression\n",
    "\n",
    "To create a powerful, summary visualization for our key findings, we will directly compare the average stress levels of depressed and non-depressed students. Our previous analyses (feature importance and association rules) strongly suggested that **Academic Pressure** and **Financial Stress** are major factors.\n",
    "\n",
    "This grouped bar chart will provide clear, visual confirmation of this relationship. By plotting the average stress scores for both groups side-by-side, we can instantly see the magnitude of the difference, creating a compelling \"wow\" factor for the project's conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a582fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Aggregate the Data in Spark ---\n",
    "# Group by the 'label' (Depression status) and calculate the average stress levels for each group.\n",
    "stress_analysis = final_df.groupBy(\"label\").agg(\n",
    "    avg(col(\"Academic Pressure\")).alias(\"Academic Pressure\"),\n",
    "    avg(col(\"Financial Stress\")).alias(\"Financial Stress\")\n",
    ")\n",
    "\n",
    "# --- 2. Convert to Pandas and Prepare for Plotting ---\n",
    "# Convert the aggregated Spark DataFrame to a Pandas DataFrame\n",
    "stress_pd = stress_analysis.toPandas()\n",
    "\n",
    "# \"Melt\" the DataFrame to transform it from a \"wide\" format to a \"long\" format,\n",
    "# which is the required input structure for a Seaborn grouped bar plot.\n",
    "plot_data = pd.melt(\n",
    "    stress_pd,\n",
    "    id_vars=['label'],\n",
    "    var_name='Stress Type',\n",
    "    value_name='Average Score'\n",
    ")\n",
    "\n",
    "# Map the numerical label to a human-readable status for the plot legend\n",
    "plot_data['Depression Status'] = plot_data['label'].map({0: 'Not Depressed', 1: 'Depressed'})\n",
    "\n",
    "# --- 3. Create the Grouped Bar Plot ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(\n",
    "    data=plot_data,\n",
    "    x='Stress Type',\n",
    "    y='Average Score',\n",
    "    hue='Depression Status',\n",
    "    palette={'Not Depressed': 'blue', 'Depressed': 'red'} # Use the same colors as before\n",
    ")\n",
    "plt.title('Average Stress Levels by Depression Status', fontsize=16)\n",
    "plt.xlabel('Type of Stressor', fontsize=12)\n",
    "plt.ylabel('Average Stress Score (0-5 Scale)', fontsize=12)\n",
    "plt.legend(title='Depression Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20230208",
   "metadata": {},
   "source": [
    "### Interpreting the \"Deep Dive\" Visualization ðŸ“Š\n",
    "\n",
    "This grouped bar chart provides clear and compelling visual evidence for one of the most significant findings of this project: **there is a strong correlation between higher stress levels and the likelihood of depression among students.**\n",
    "\n",
    "#### Key Observations:\n",
    "* **Academic Pressure:** The chart shows a stark difference between the two groups. Students classified as **Depressed** (red bar) report a much higher average academic pressure score (around 3.7) compared to those who are **Not Depressed** (blue bar, around 2.4).\n",
    "* **Financial Stress:** The same strong pattern holds true for financial stress. The **Depressed** group reports a significantly higher average score (around 3.6) than the **Not Depressed** group (around 2.5).\n",
    "\n",
    "**Conclusion:** This single visualization effectively encapsulates the insights from both our feature importance analysis and our association rule mining. It powerfully demonstrates that students experiencing higher levels of academic and financial pressure are far more likely to also report being depressed. This serves as an excellent summary and \"wow factor\" for the project's conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8bee3",
   "metadata": {},
   "source": [
    "### Additional Visualization: The Impact of Family History\n",
    "\n",
    "To supplement our primary finding on stress, we will also visualize the impact of another key variable identified in our earlier analyses: `Family History of Mental Illness`. The following 100% stacked bar chart illustrates the difference in depression rates between students with and without a family history of mental illness, providing another layer of insight into the factors at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Aggregate the Data in Spark ---\n",
    "# Group by family history and depression status to get counts\n",
    "family_history_counts = final_df.groupBy(\"Family History of Mental Illness\", \"label\").count()\n",
    "\n",
    "# --- 2. Convert to Pandas and Calculate Proportions ---\n",
    "family_history_pd = family_history_counts.toPandas()\n",
    "\n",
    "# Calculate the total number of students for each family history category (Yes/No)\n",
    "totals = family_history_pd.groupby('Family History of Mental Illness')['count'].sum()\n",
    "\n",
    "# Calculate the percentage for each group\n",
    "family_history_pd['percentage'] = family_history_pd.apply(lambda row: 100 * row['count'] / totals[row['Family History of Mental Illness']], axis=1)\n",
    "\n",
    "# --- 3. Create the 100% Stacked Bar Plot ---\n",
    "# Pivot the data to get it into the right shape for plotting\n",
    "pivot_df = family_history_pd.pivot(index='Family History of Mental Illness', columns='label', values='percentage')\n",
    "\n",
    "# Create the plot\n",
    "pivot_df.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=['blue', 'red'], # Not Depressed (0), Depressed (1)\n",
    "    figsize=(10, 7)\n",
    ")\n",
    "\n",
    "plt.title('Impact of Family History on Depression Rate', fontsize=16)\n",
    "plt.xlabel('Family History of Mental Illness', fontsize=12)\n",
    "plt.ylabel('Percentage of Students', fontsize=12)\n",
    "plt.xticks(rotation=0) # Keep the x-axis labels horizontal\n",
    "plt.legend(title='Depression Status', labels=['Not Depressed', 'Depressed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a2474",
   "metadata": {},
   "source": [
    "### Interpreting the Family History Visualization ðŸ§¬\n",
    "\n",
    "This 100% stacked bar chart effectively visualizes the impact of having a family history of mental illness on the likelihood of a student being depressed.\n",
    "\n",
    "#### Key Observations:\n",
    "* **For students with NO family history:** The chart shows that the depression rate is approximately **56%** (the red portion of the \"No\" bar).\n",
    "* **For students with a family history:** The depression rate increases to approximately **61%** (the red portion of the \"Yes\" bar).\n",
    "\n",
    "**Conclusion:** This chart confirms that there is a **positive correlation between having a family history of mental illness and the rate of depression** in this dataset. While the effect is not as dramatic as the differences seen with the stress-level chart, it is still a significant finding that validates one of the key features identified in our earlier analyses. This serves as a great secondary insight for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536ae75",
   "metadata": {},
   "source": [
    "## Part 5: Conclusion & Future Work\n",
    "\n",
    "This project successfully leveraged Apache Spark and its MLlib library to conduct a comprehensive analysis of student mental health. By implementing four distinct machine learning functionalities, we were able to extract a wide range of valuable insights from the dataset.\n",
    "\n",
    "### 5.1 Summary of Findings \n",
    "\n",
    "Our analysis yielded several key findings:\n",
    "* **Strong Predictive Power for Depression:** Our Logistic Regression model proved to be highly effective at predicting student depression, achieving an **AUC of 0.92** and a **Recall of 88.24%**. The feature importance analysis revealed that a student's **city of residence** was the most powerful predictor.\n",
    "* **Stress as a Key Factor:** Both the **Association Rule Mining** and the **\"Deep Dive\" Visualisation** confirmed a strong, statistically significant link between high levels of **Academic and Financial Stress** and the likelihood of depression and suicidal thoughts.\n",
    "* **Identifiable At-Risk Profiles:** The **KMeans Clustering** analysis successfully identified distinct student profiles. The \"At-Risk\" groups were clearly characterized by higher average stress levels, validating the findings from our other models.\n",
    "* **Poor Predictability of Academic Performance:** Our **Linear Regression** model demonstrated that the available features were very poor predictors of a student's CGPA, achieving an **RÂ² of only 0.0168**. This is a valuable finding in itself, indicating that academic performance is likely influenced by factors not captured in this dataset.\n",
    "\n",
    "### 5.2 Project Reflection on Cloud Technologies \n",
    "\n",
    "This project demonstrated the immense value of using a distributed computing framework like **Apache Spark**. For a dataset of nearly 28,000 records with numerous features, Spark allowed for:\n",
    "* **Scalability:** All data loading, pre-processing, and model training tasks were performed efficiently. This architecture could seamlessly scale to handle a dataset with millions of student records without requiring significant code changes.\n",
    "* **Unified Framework:** Spark provided a single, unified API to handle everything from initial data ingestion (Spark SQL) to complex machine learning (Spark MLlib). This streamlined the entire workflow and is a significant advantage over traditional tools that would require separate libraries for each step.\n",
    "\n",
    "### 5.3 Future Work \n",
    "\n",
    "While this project was successful, there are several exciting avenues for future work:\n",
    "* **Model Deployment:** The trained Logistic Regression model could be deployed as a real-time web application or API. This could serve as the backend for a student wellness tool that provides early-intervention recommendations.\n",
    "* **Advanced Algorithms:** More complex models, such as Gradient-Boosted Trees or Random Forests, could be trained to potentially improve predictive accuracy and capture non-linear relationships in the data.\n",
    "* **Deeper Feature Engineering:** Incorporating additional data sources, such as social media activity or university attendance records, could provide new features that might improve the predictability of both depression and academic performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
