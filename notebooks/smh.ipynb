{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b6c45",
   "metadata": {},
   "source": [
    "# Predicting Student Depression: A Big Data Analytics Approach with Apache Spark\n",
    "\n",
    "- **Author:** David Araba\n",
    "- **Student ID:** 48093143\n",
    "- **Course:** INFS3208 - Cloud Computing\n",
    "- **Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8255577",
   "metadata": {},
   "source": [
    "## 1. Introduction & Project Goals\n",
    "\n",
    "### 1.1. Objective\n",
    "The primary objective of this project is to develop and evaluate a suite of machine learning models using Apache Spark to predict the likelihood of depression among students. By leveraging a comprehensive dataset that includes demographic, academic, and lifestyle factors, I aim to identify key indicators associated with mental health challenges in an academic environment.\n",
    "\n",
    "### 1.2. Significance\n",
    "Student mental health is a growing concern globally. The pressures of academic life, combined with financial and social stressors, can significantly impact a student's well-being and academic performance. This project is important because it seeks to create a data-driven framework that could potentially identify at-risk students, enabling educational institutions to offer timely and targeted support. By using scalable cloud computing technologies, we can build a foundation for a system capable of handling large-scale, real-world student data, moving from reactive to proactive mental wellness strategies. [cite_start]This aligns with the need for modern solutions that traditional computing can struggle to scale effectively[cite: 1].\n",
    "\n",
    "### 1.3. Technical Stack\n",
    "This project will be implemented using the following technologies:\n",
    "* **Language:** Python 3.x\n",
    "* **Core Engine:** Apache Spark (via PySpark)\n",
    "* **Libraries:**\n",
    "    * **Spark MLlib:** For building scalable machine learning pipelines.\n",
    "    * **Pandas:** For initial data handling and manipulation.\n",
    "    * **Matplotlib & Seaborn:** For data visualisation and result interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbdb",
   "metadata": {},
   "source": [
    "## 2. Project Architecture & Workflow\n",
    "\n",
    "\n",
    "### 2.1. Workflow Description\n",
    "This project follows a standard big data analytics workflow, as depicted in the diagram below. The process begins with the ingestion of four separate but related data files into the Spark environment. These datasets are then joined and pre-processed to create a unified, analysis-ready master dataset. Subsequently, this dataset is used to train and evaluate four distinct machine learning functionalities as required by the project specification: classification, regression, clustering, and association rule mining. The final insights and model performance metrics are then visualised to provide clear, interpretable results.\n",
    "\n",
    "### 2.2. Architecture Diagram\n",
    "This diagram illustrates the project's workflow from data source to final analysis. It explicitly shows the use of multiple data sources and the application of various Spark MLlib functionalities, fulfilling the key project requirements.\n",
    "\n",
    "\n",
    "### 2.3. Architecture Explanation\n",
    "The above workflow diagram demonstrates a comprehensive big data analytics pipeline designed for scalable student mental health analysis. The process begins with four distinct data sources being ingested simultaneously into the Spark environment, leveraging Spark's distributed processing capabilities to handle large-scale datasets efficiently. The unified master dataset is then processed through feature engineering pipelines before being split for model training and evaluation. The four ML functionalities operate in parallel, each addressing different aspects of student mental health prediction and analysis. This architecture showcases the power of cloud computing technologies in handling complex, multi-dimensional data analysis tasks that would be challenging with traditional single-machine approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27c77a",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "This section prepares the notebook's environment. The first part imports all necessary libraries for the project, including `pyspark` for distributed data processing, `pyspark.ml` for machine learning, and `matplotlib` for visualisation. The second part initialises the `SparkSession`, which is the essential entry point to all of Spark's functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460345b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 0: Project Initialisation & Overview\n",
    "# 3. Environment Setup\n",
    "\n",
    "# 3.1. Import All Necessary Libraries\n",
    "# ---\n",
    "\n",
    "# Spark libraries for session management, data manipulation, and ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, avg, mean, array\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "\n",
    "# Standard Python libraries for data handling and plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# 3.2. Initialise Spark Session\n",
    "# ---\n",
    "# Create a SparkSession, which is the entry point to any Spark functionality.\n",
    "# - appName: Sets a name for the application, which will appear in the Spark UI.\n",
    "# - getOrCreate(): Gets an existing SparkSession or, if there is none, creates a new one.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StudentMentalHealthPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark session created successfully. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8994e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224c780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
